\section{understanding this report}
In this report, each function benchmarked by criterion is assigned
a section of its own.  In each section, we display two charts, each
with an <i>x</i> axis that represents measured execution time.
These charts are active; if you hover your mouse over data points
and annotations, you will see more details.</p>

The chart on the left is a

\href{http://en.wikipedia.org/wiki/Kernel_density_estimation}{kernel
density estimate} (also known as a KDE) of time
measurements. This graphs the probability of any given time
measurement occurring. A spike indicates that a measurement of a
particular time occurred; its height indicates how often that
measurement was repeated.

The chart on the right is the raw data from which the kernel
density estimate is built.  Measurements are displayed on
the $y$ axis in the order in which they occurred.
   
Under the charts is a small table displaying the mean and standard
deviation of the measurements.  We use a statistical technique
called
the 
\href{http://en.wikipedia.org/wiki/Bootstrapping_(statistics)}{bootstrap}
to provide confidence intervals on our estimates of these values.
The bootstrap-derived upper and lower bounds on the mean and
standard deviation let you see how accurate we believe those
estimates to be. (Hover the mouse over the table headers to see
the confidence levels.)
   
A noisy benchmarking environment can cause some or many
measurements to fall far from the mean. These outlying
measurements can have a significant inflationary effect on the
estimate of the standard deviation. We calculate and display an
estimate of the extent to which the standard deviation has been
inflated by outliers.
   
   
These reports were created using the
\href{http://hackage.haskell.org/package/criterion}{criterion}
benchmark execution and performance analysis tool.
Criterion is developed and maintained
by \href{http://www.serpentine.com/blog/}{Bryan O'Sullivan}
