\chapter{Introduction}
Editors normally have regular-expression based parsers, which are efficient and
robust, but lack in precision: they are unable to recognize complex structures.
Parsers used in compilers are precise, but typically not robust: they fail to
recover after an error. They are also not efficient for editing purposes,
because they have to parse files from the beginning, even if the user makes
incremental changes to the input. More modern IDEs use compiler strength
parsers, but they give delayed feedback to the user. Building a parser with good
characteristics is challenging: no system offers such a combination of
properties.

In order to implement a parser with the characteristics described above; robust,
precise and efficient; a lexer that has the same properties is needed. This
project aims to implement such a lexer.

\section{Scope of work}
Existing lexical analyzers are sequential. When the text is updated the lexer
must start the lexical analysis from the beginning. The goal of this project is
to create an algorithm that, after an update to the text, only needs to
recalculate the update and the part of the result effected by the update. The
recalculation should have time complexity $\Theta log(n)$ in order to be run in
real time, for example in a text editor with immediate update.

\Cref{chap:lexer} gives a general understanding of how lexical analyzers work.
\Cref{chap:divconqlexer} presents tools needed for a divide and conquer
implementation to work. \Cref{chap:divconqlexer} also gives an overview of the
ideas behind a divide and conquer lexer in order to give enough understanding
for the algorithm this report proposes. A robust implementation of the algorithm
is presented in \cref{chap:imp} with explanations on how different cases are
handled. Tests for preciseness, time performance and space performance are
explained and their corresponding result are presented in \cref{chap:result}.
A discussion on where a divide and conquer lexer is useful is presented in
\cref{chap:disc}.

\section{Related Work}
This project revolves around the idea of using incremental regular expressions.
Piponi wrote a blog post about how to implement incremental regular
expressions using finger trees \cite{blog}. The solution to matching regular expressions
incrementally in the blog post gives a good starting point to this project,
however a lexer does not match a string against one expression. A lexer matches a
string against a set of regular expressions and returns which expressions where
matched and in what order rather then answering if the string matched the
expressions. The ``longest match'' rule for lexers further complicates the issue
\cite{blog}.

Bernardy and Claessen \cite{bernardyefficient2013} wrote a paper titled 
``Efficient Divide-and-Conquer Parsing of Practical Context-Free Languages''
that describes an efficient parallel parser built on Valiants algorithm
\cite{valiantgeneral1975}. The paper proves that for a defined set of input
the complexity for the parser will be $\Theta log^3(n)$.
Since the implementation of the parser is done in BNFC \cite{bnfc} it uses a
sequential lexer generated by alex \cite{alex}. The lexical analyzer this
project purposes is a divide and conquer solution which could be integrated to
the parser proposed by Bernardy and Claessen to get a divide and conquer
solution from the programming code to the result of the parser.
