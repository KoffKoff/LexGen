\chapter{Divide-and-Conquer Lexer \label{chap:divconqlexer}}
An incremental divide and conquer lexer works by dividing the sequence to be
lexically analyzed, into small parts and analyzes them; and then combines them.
In the base case the lexical analysis is done on a single character. The
conquer step then combines the smaller tokens into as large tokens as possible.
The end result is a sequence of tokens that represent the code. How this is done
is described in this chapter.

\section{Divide and Conquer in General \label{sec:divconq}}
This section gives an idea of how the Divide and Conquer algorithm
works in general, before addressing in detail how to apply it to lexing. It
describes the power of divide and conquer in terms of executing time and how
laziness can be applied to these algorithms.

\subsection{The Three Steps}
The general idea of a divide and conquer algorithm is to divide a problem into
smaller parts, solve them independently and then combine the results. A Divide
and Conquer algorithm always consists of a pattern with these three steps
\cite{Goodrich}.
\begin{description}
\item[Divide:] If the input size is bigger than the base case then divide the
input into subproblems. Otherwise solve the problem using a straightforward
method.
\item[Recur:] Solve the subproblems by recursively call itself with each
sub-problem as argument.
\item[Conquer:] Given the solutions to the subproblems, combine the results to
solve the original problem.
\end{description}

\subsection{Associative Function}
An associative function, or operator, is a function that does not care in what
order it is applied. An example of such a function is addition $(+)$ of numbers,
which is associative since it has the property in \cref{assprop}, that is,
$a+(b+c)=(a+b)+c$.

In divide and conquer algorithms this is essential. In the divide step of the
divide and conquer algorithm, there is no certain order of how the subproblems
are going to be divided. This means that the order the subproblems are being
conquered can not have an impact on the algorithm, hence the conquer step must be
associative.

\begin{example}[Associativity of the conquer step]\label{assprop}
Let $f(x,y)$ be the conquer function, where $x$ and $y$ are of the same type as
the result of $f$, then:
\begin{center}
$f(x,f(y,z)) = f(f(x,y),z)$
\end{center}
Otherwise the algorithm can give different results for the same data.
\end{example}

\subsection{Time Complexity}
To calculate the running time of any divide and conquer algorithm the master
method can be applied \cite{Cormen}. This method is based on the following
theorem.

\begin{theorem}[Master Theorem \cite{Cormen}, \label{MasterTheo}
 as described by \cite{bernardyefficient2013}] $ $ \\
Assume a function $T_n$ constrained by the recurrence
\begin{center}
$T_n = {\alpha}T_{\frac{n}{\beta}}+ f(n)$
\end{center}
(This is typically the equation for the running time of a divide and conquer
algorithm, where $\alpha$ is the number of subproblems at each recursive step,
$n/\beta$ is the size of each subproblem, and $f(n)$ is the running time of
dividing up the problem space into $\alpha$ parts, and combining the results
of the subproblems together.)\\
If we let $e = \log_\beta \alpha$, then
\begin{center}
\begin{tabular}{r c l l}
1. $T_n$ & $=$ & $O(n^{e})$ &  if $f(n) = O(n^{e - \epsilon})$ and $\epsilon > 0$\\
2. $T_n$ & $=$ & $\Theta(n^{e} \log n)$ & if $f(n) = \Theta(n^e)$\\
3. $T_n$ & $=$ & $\Omega(f(n))$ & \begin{minipage}[t]{0.6 \columnwidth}
  if $f(n) = \Omega(n^{e+\epsilon})$ and $\epsilon > 0$
  and $\alpha \cdot f(n/\beta) \leq c \cdot f(n)$
  where $c < 1$ and all sufficiently large $n$
  \end{minipage}
\end{tabular}
\end{center}
\qeda
\end{theorem}

\subsection{Hands on Example}
The divide and conquer pattern can be performed on algorithms that solves
different problems. A general problem is sorting, or more precisely sorting a
sequence of integers. This example shows merge-sort.

\begin{description}
\item[Divide:] The algorithm starts with the divide step. Given the input $S$
the algorithm will check if the length of $S$ is less then or equal to 1.
\begin{itemize}
\item If this is true, the sequence is returned. A sequence of one or zero
elements is always sorted.
\item If this is false, the sequence is split into two equally big sequences,
$S_1$ and $S_2$. $S_1$ will be the first half of $S$ while $S_2$ will be the
second half.
\end{itemize}
\item[Recur:] The next step is to sort the subsequences $S_1$ and $S_2$. The
sorting function sorts the subsequences by recursively calling itself twice with
$S_1$ and $S_2$ as arguments respectively.
\item[Conquer:] Since $S_1$ and $S_2$ are sorted combining them into one sorted
sequence is trivial. This process is what is referred to as merge in merge-sort.
The resulting sequence of the merge is returned.
\end{description}
Algorithm~\ref{Alg:MergSort} shows a more formal definition of merge-sort.

\begin{algorithm}
\DontPrintSemicolon
\KwData{Sequence of integers $S$ containing $n$ integers}
\KwResult{Sorted sequence $S$}
\If {$length(S) \leq 1$}{
  \Return $S$ \;
}
\Else {
  $(S_1,S_2) \gets splitAt(S,n/2)$ \;
  $S_1 \gets MergeSort(S_1)$\;
  $S_2 \gets MergeSort(S_2)$\;
  $S \gets Merge(S_1, S_2)$\;
  \Return $S$
}
\caption{MergeSort}
\label{Alg:MergSort}
\end{algorithm}

Given the merge-sort algorithm, time complexity can be calculated as follows
using the master method. There are $2$ recursive calls and the subproblems are
$1/2$ of the original problem size, so $\alpha=2$ and $\beta=2$. To merge the
two sorted subproblems the worst case is to check every element in the two list,
$f(n) = 2 \cdot n/2 = n$.
\begin{center}
$T(n) = 2T(n/2) + n$\\
$e=\log_\beta\alpha=\log_2 2=1$
\end{center}
Case 2 of the \cref{MasterTheo} applies, since
\begin{center}
$f(n) = \Theta(n)$
\end{center}
So the solution will be:
\begin{center}
$T(n) = \Theta(n^{\log_2 2} \cdot \log n) = \Theta(n \cdot \log n)$
\end{center}

\subsection{Incremental Computing}
For an algorithm to be incremental means that when a point in the data source
is updated, the algorithm tries to save time by only update the direct effected
path in the data source \cite{incrementalDef}. The \cref{fig:incUp} illustrate
the updated nodes in a tree structured data source.

\begin{figure}[!htp]
\centering
  \begin{tikzpicture}[level/.style={sibling distance=60mm/#1, align=center, text centered}]
\node [leaf,red] (z){$r'$}
  child { node[leaf,red] (a) {$c'$}
    child {node[leaf] (b) {$a$}
      child {node[leaf] (c) {$-$}} 
      child {node[leaf] (d) {$-$}}
    }
    child {node[leaf,red] (e) {$b'$}
      child {node[leaf] (f) {$x$}}
      child {node[leaf,red] (g) {$y'$}}
    }
  }
  child {node[leaf] (i) {$d$}
    child {node[leaf] (j) {$-$}
      child {node[leaf] (k) {$-$}}
      child {node[leaf] (l) {$-$}}
    }
    child {node[leaf] (m) {$-$}
      child {node[leaf] (n) {$-$}}
      child {node[leaf] (o) {$-$}}
    }
  };
\end{tikzpicture}
\caption{When the node $y$ changed recomputed nodes are marked with a $'$. \label{fig:incUp}}
\end{figure}

For a divide and conquer lexer this means to only recompute the changed token
and the token to the right of the changed token. This is done recursively until
the root of the tree is reached. The expected result of this would be that when
a character is added to the code of 1024 tokens, instead of recalculating all
the 1024 tokens the lexer only needs to do 10 recalculations, since
$log_2 1024 = 10$. This can be explained by the \cref{MasterTheo}.

Only one branch in the tree will be followed at every level and the problem is
already divided. Therefore the parameters will be set to:
\begin{center}
$\alpha = 1$, $\beta = 2$ and $f(n) = 1$.
$e=log_\beta\alpha=\log_2 1=0$
\end{center}
Case 2 of the \cref{MasterTheo} applies, since
\begin{center}
$f(n) = \Theta(n^e)$
\end{center}
The complexity is therefore:
\begin{center}
$T(n) = \Theta(n^e \cdot \log n) = \Theta(\log n)$
\end{center}

\newpage

\section{Fingertree}
A fingertree is a tree structure that is built to make access to the beginning
and end of a collection easy. In the following section fingertrees are
explained. The code examples are simplified for demostrative purposes, for
instance in real implementations lists are not used since they do not give good
performance for what fingertrees are designed for.

\subsection{Structure of Fingertrees}
To achieve fast access to the begining and end of the tree the leaves for the
1-4 first elements and 1-4 last elements are placed in the root of the tree,
these are called fingers. The rest of
the elements constitutes the spine which is another fingertree, with one
difference, instead of having the first 1-4 elements the second level will
instead have two 2-3 tree of depth 2 at the beginning and end of the tree. The
third level will have 2-3 trees of depth 3 and for level $n$ the 2-3 trees in
the beginning and end will have depth $n$. An illustration of how a fingertree
can look can be seen in \cref{fig:fingertree} \cite{fingertree}.

\begin{figure}[!h]
\centering
\begin{tikzpicture}[level distance=2cm, sibling distance = 2cm]
    \node[blackbranch] {}
        child { node[leaf] {1} }
        child { node[leaf] {2} }
        child[level distance=2cm, sibling distance=2.5cm, grow=down]
          { node[blackbranch] {}[level distance=2cm]
            child { node[branch] {} [sibling distance=1cm]
                child { node[leaf] {3} }
                child { node[leaf] {4} }
            }
            child { node[branch] {} [sibling distance=1cm]
                child { node[leaf] {5} }
                child { node[leaf] {6} }
            }
            child[level distance=2cm, grow=down]
              { node[blackbranch, fill=white] {}}
            child { node[branch] {} [sibling distance=1cm]
                child { node[leaf] {7} }
                child { node[leaf] {8} }
                child { node[leaf] {9} }
            }
            child { node[branch] {} [sibling distance=1cm]
                child { node[leaf] {10} }
                child { node[leaf] {11} }
            }
        }
        child { node[leaf] {12} }
        child { node[leaf] {13} }
        child { node[leaf] {14} }
    ;
\end{tikzpicture} 
\caption{Illustration of Fingertree \label{fig:fingertree}}
\end{figure}

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/HaskellFingerTree.hs}
\caption{Definition of the Fingertree data type \label{fig:DataTypeFingertree}\cite{fingertree}}
\end{figure}

As can be seen in figure \cref{fig:DataTypeFingertree} there are three constructors
for a fingertree, there are the trivial cases for an empty tree and for a tree
with one element. The last constructor calls itself with $Node~a$ instead of
just $a$. This is what determines the depth of the 2-3 tree on each side of the
spine. The reasons for the first level of the tree having fingers of size 1-4 is
because of insertion and deletion which is covered in \cref{insertDelete} \cite{fingertree}.

Accessing an element at place $d$ in the tree will take $O(\log(min(d,n-d)))$. This
is because the closer to the end of the fingertree the element is the closer the
surface it is. This in turn gives the time complexity of accessing an element,
which in worst case is $O(\log n)$ and for the first and last element is $O(1)$
\cite{fingertree}.

\subsection{Insertion and Deletion \label{insertDelete}}
The fingertree described so far can only handle insertion and deletion to the
begining and end of the tree, for more functionality, measurments are needed
which is covered in \cref{sub:measurement}.
The fingers in a tree have two different states called dangerous and safe. The
safe states is when there are 2 to 3 elements in the finger and when the finger
is safe an insertion or deletion from the tree will not be anything more then an
insertion or deletion in that finger \cite{fingertree}.

When a finger has 1 or 4 elements the finger is called dangerous. In this case
there might be implications down the spine. The first case is when there is an
insertion into a finger that has 4 elements, in this case there are 5 elements
that is assigned to the same level. If the insertion was done to the end of the
tree the last 2 elements will be the new finger which is then a safe finger. The
first 3 elements are used to create a new $Node3$ which is passed down the spine
as a single element to be inserted at the next level. Inserting an element to
the end of a tree can be seen in \cref{fig:AddLast}, conversly adding an element
to the begining of a tree is the mirror to this function \cite{fingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeInfixl.hs}
\caption{Adding an element to the end of the sequence \label{fig:AddLast}\cite{fingertree}}
\end{figure}

The other dangerous operation is when a deletion from a finger of size 1 is
done. In this case, if the deletion is made to the end of the tree, the last
element of the end finger in the spine is deleted and used as the finger for the
level where the the finger would have been empty. Since the element in the
finger below will either be a $Node2$ or a $Node3$ the new finger will be safe.
Deletion of an element at the end or begining of a tree is implemented
similiarly to $head$ for lists, the functions $viewL$ which returns the first
element and the rest of the tree can be seen in \cref{fig:Viewl} \cite{fingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeViewL.hs}
\caption{Adding an element to the end of the sequence \label{fig:Viewl} \cite{fingertree}}
\end{figure}

Since the dangerous states propagate actions down the spine insertion and
deletion will not take $O(1)$ time in the worst case. Since
each new level will take at most $O(1)$ time the insertion or deletion of an
element will in the worst case take $O(\log n)$ time. However since 3 of 4
operations are safe for insertion and deletion respectivly the expected time
consumption for an insertion or deletion at the begining or end of the tree will
be $O(1)$. This is because each operation on a dangerous finger will render it
safe for the next time it is accessed \cite{fingertree}.

\subsection{Concatenation of Fingertrees}
When 2 fingertrees are concatenated there are a number of different cases which
can occur. To begin with, when a concatenation of two trees is done a function
called $app3$ is called with the two trees and an empty list of ``between''
elements. As can be seen in \cref{fig:concat} there are 4 trivial cases, the first
two is when either tree is empty in which case the ``between'' elements is
added to the nonempty tree. The other two are when there is exactly one element
in one of the trees in which case that element is added to the other tree after
the ``between'' elements \cite{fingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeConcatFunc.hs}
\caption{Concatenation function for Fingertree \label{fig:concat}\cite{fingertree}}
\end{figure}

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeAppend.hs}
\caption{Help function for inserting a list of element into a fingertree
  \label{fig:reduceAppend}\cite{fingertree}}
\end{figure}

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeNodesFunc.hs}
\caption{Help function for transforming a list of element into a list of Nodes
  \label{fig:nodesHelp}\cite{fingertree}}
\end{figure}

In the last case two trees of more then one elements
are concateneted. In this case, a new tree is created which has the first
finger set as the first finger from the first tree and the last finger set as
the last finger from the second tree. The spine will be created by calling
$app3$ recursivly with the spine from the first tree as the first tree, the last
finger of the first tree plus ``between'' elements plus the first finger of the
second tree as the new ``between'' elements and the spine from the second tree
as the second tree \cite{fingertree}.

The time complexity for concatenation can be reasoned as follows. As can be seen
in \cref{app3 ref} the only operation that is run recursivly is $nodes$. $nodes$
will run in $O(1)$ time since the most amount of arguments passed to it will be
12 in which case 4 $Node3$ elements will be returned. Since each recursive step
takes at most $O(1)$ time and the function terminates when the bottom of the
shallower tree has been reached the total time to concatenate two trees is
$\theta(\log(min(n,m)))$ where $n$ and $m$ is the size of the trees being
concatenated \cite{fingertree}.

\subsection{Measurements \label{sub:measurement}}
To make fingertrees useful for a divide and conquer lexer measures of the tree
needs to be added. A measure of a tree may for example be how many elements is
in the tree or as will be shown later in the report, the lexed tokens of the
text in a tree. To implement measures time effiecently the data type that is
choosen should be a monoid. A monoid is in abstract algebra a set $S$ and an
operator $(<>)$ which satisfies the following rules \cite{fingertree}:

\begin{description}
\item[Closure] $\forall a,b \in S: a <> b \in S$
\item[Associativity] $\forall a,b,c \in S: (a <> b) <> c = a <> (b <> c)$
\item[Identity element] $\exists e \in S: \forall a \in S: e <> a = a <> e = a$
\end{description}

An example of a monoid is the natural numbers which under addition form a monoid
where the identity element is $0$.
Using the class system in Haskell measures are defined as in \cref{fig:measure}.
$mempty$ will henceforth be used as the identity element in examples and
defenitions.

\begin{figure}[h!]
\lstinputlisting[language=Haskell, mathescape=true]{examples/ClassMeasure.hs}
\caption{Definition of the Measure class \label{fig:measure} \cite{fingertree}}
\end{figure}

Since the measure is a monoid, when two trees are concatenated the measure of 
the new tree is simply $measure ~ tr1 <> measure ~ tr2$.
The $Digit$ data type in the trees are always of constant size, however the
elements in $Digit$ are of type $Node$ $a$ which grows with the depth of the tree.
Because of this the $Node$ $a$ data type is also measured as can be seen in
\cref{fig:measureNode} \cite{fingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell, mathescape=true]{examples/Node2-3Measure.hs}
\caption{Measure of the data type Node \label{fig:measureNode} \cite{fingertree}}
\end{figure}

With the measurement modifcation to the fingertree the datastructure is almost
the same, with the addition of measures at certain places as can be seen in
\cref{fig:measuredFingerTree} where $deep$ is used as the new constructor.
Because of how fingertrees are implemented the type of the elements will change,
in the begining it is $a$ at the second level it is $node$ $v$ $a$. However the
measure will always be of type $v$ \cite{fingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell, mathescape=true]{examples/FingerTreeMeasure.hs}
\caption{Fingertrees Measure function \label{fig:measureFingerTree} \cite{fingertree}}
\end{figure}

Fingertrees offers a data structure where the time complexity for operations on
the tree scales logarithmically with the size of the tree. Because of this and
the fact that operations on the measure in the fingertree has constant time it
makes the data structure suitable for a divide and conquer lexer.

\section{Divide and Conquer Lexing in General}
In \cref{sec:divconq} the general divide and conquer algorithm was covered. This
section covers the general data structures and algorithms for an incremental
divide and conquer lexer.

\subsection{Tree structure}
The incremental divide and conquer lexer should use a structure where the
code-lexemes can be related to its tokens, current result can be saved and
easily recalculated. A divide and conquer lexer should therefore use a tree
structure to save the lexed result in. Since every problem can be divided into
several subproblems, until the base case is reached. This is cleraly a tree
structure of solutions, where a leaf is a token for a single character, and the
root is a sequence of all tokens in the code.  

\subsection{Transition map}
When storing a result of a lexed string it is a good idea to store more then
just the tokens. In particular the in and out states are needed when combining
the lexed string with another string. We will henceforth refer to this as a
$transition$.
\begin{lstlisting}[language=Haskell]
type Transition = (State,[Token],State)
\end{lstlisting}
Since the lexer does not know if the current string is a prefix of the entire
code or not it can not make any assumptions on the in state. Because of this the
lexer needs to store a transition for every possible in state, we will henceforth
refer to this as a \emph{transition map}.
\begin{lstlisting}[language=Haskell]
type Transition_map = [Transition]
\end{lstlisting}
\subsubsection{The Base Case}
When the lexer tries to lex one character it will create a transition
map using the DFA for the language. It will for each state create a transition
that has the state as in state, a list containing the character as the only
token and by using the DFA, lookup what out state the transition should have.
For the character 'o' part of a transition map might look like \cref{fig:bascas}.

In \cref{fig:bascas}, \cref{fig:conq} and \cref{fig:longconq} the first number
refers to the in state, the middle part is the sequence of tokens and the second
number is the out state, that can be accepting.

\begin{figure}[!ht]
\begin{center}
$\left[\begin{array}{ccc}
0&['o']&Accepting 5\\
1&['o']&1\\
10& &NoState\\
\end{array}\right]$
\caption{The Base Case for divide and conquer lexing \label{fig:bascas}}
\end{center}
\end{figure}

$NoState$ transition is used to tell the lexer that using that particular 
transition will result in a lexical error. For reasons being covered later in
this section, they can not be discarded.
%\cref{longmatch}

\subsubsection{Conquer Step}
The conquer step of the algorithm is to combine two transition maps into one
transition map. This is done by, for every transition in the left transition
map, combining the transition with the transition in the right transition map
that has the same in state as the left transitions out state. This can be
described by the function in \cref{fig:mergetransmap} where $map1$ and $map2$
refers to the first and second transition map.

\begin{figure}
\begin{lstlisting}
merge :: Transition_map -> Transition_map -> Transition_map
merge map1 map2 = [(i,t1><t2,o) | (i,t1,o1) <- map1, (i2,t2,o) <- map2, o1==i2]
\end{lstlisting}
\caption{Function for merging two transition maps into one transition map
  \label{fig:mergetransmap}}
\end{figure}

The most general case is a naive lexer that takes the first accepting state it
can find. When two transitions are combined there are two different outcomes:
\begin{description} 
  \item[Append:]If the out state of the first transition is accepting, the
    sequence in the transition that starts in the starting state of the second
    transition map will be appended to the first.
\begin{lstlisting}[language=Haskell, mathescape=true]
appendTokens :: Tokens -> Tokens -> Tokens 
appendTokens tokens1 tokens2 = tokens1 >< tokens2
\end{lstlisting}

%create two tokens if the
%    out state of the first list is accepting then the second list will be appended
%    to the first list.
  \item[Merge:]If the out state of the first transition is not accepting, the
    transition in the second transition map with the same in state as the out
    state of the first transition will be used. The last token of the sequence
    from the first transition will be merged with the first token in the second
    transition into one token and put between the two sequences.\\

\begin{lstlisting}[language=Haskell, mathescape=true]
mergeTokens :: Tokens -> Tokens -> Tokens 
mergeTokens tokens1 tokens2 = prefix1 |> newToken >< suffix2
  where prefix1 |> token1 = tokens1
        token2 <| suffix2 = tokens2
        newToken          = token1 `combinedWith` token2
\end{lstlisting}

%The other case is when the first list of tokens does not end
%    in an accepting state. In this case the lexer will try to find an in state in
%    the second list that is the same as the out state of the first transition.
\end{description}
For both the cases the in state of the first transition will be the new in state
and the out state of the second transition will be the new out state. An example
of both cases is shown in \cref{fig:conq}.

\begin{figure}[!ht]
\begin{center}
$\left[\begin{array}{ccc}
0&['o']&Accepting 5\\
1&['o']&1\\
\end{array}\right] `combineTokens`
\left[\begin{array}{ccc}
0&['~~']&Accepting 2\\
1&['~~']&1\\
\end{array}\right] =
\left[\begin{array}{ccc}
0&['o','~~']&Accepting 2\\
1&['o~~']&1\\
\end{array}\right]$
\caption{The Conquer step for Divide and Conquer lexing\label{fig:conq}}
\end{center}
\end{figure}

This will not work as a lexer for most languages since the longest match rule is
not implemented. For example, it will lex a variable to variables where the
length is a single character, for example ``os'' will be lexed as two tokens,
``o'' and ``s''. To solve this some more work is needed.

\subsubsection{Longest Match}\label{longmatch}
To ensure that only the longest token is returned some stricter rules for
combinations are needed. Firstly, if two transitions can be combined without
having the outgoing state $NoState$ then merge those transition. When two
transitions are merged the last token of the left transition is merged with
the first token of the right transition into one token. Secondly, If the
combination of two transtitions would yield $NoState$, the transitions are
appended instead. When two transitions are appended the right transition
starting from the starting state is appended to the left transition. As can be
seen in \cref{fig:longconq} 

\begin{figure}[!ht]
\begin{center}
$\left[\begin{array}{ccc}
0&['o','~~']& Accepting 2\\
1&['o~~'] & 1\\
\end{array}\right] `combineTokens` 
\left[\begin{array}{ccc}
0&['/','*']&Accepting 4\\
1&['*/']&Accepting 3\\
2& &NoState\\
\end{array}\right] =
\left[\begin{array}{ccc}
0&['o','~~','*','/']&Accepting 4\\
1&['o~~*/']& Accepting 3\\
\end{array}\right]$
\caption{The Conquer step when the longest match rule is applied\label{fig:longconq}}
\end{center}
\end{figure}

When two transitions are appened another rule needs to be accounted for. If the
last token of the first transition does not end in an accepting state a lexical
error is found. How lexical errors are handled and stored is explained in
\cref{sec:lexerr}.

\subsection{Lexical Errors}\label{sec:lexerr}
Even though lexical errors can not halt the lexer it is still usefull to keep
them since they tell the user what is wrong. In an incremental lexer there are
different ways this can be achieved. The simplest way is to store the lexical
error, instead of the tokens and outgoing state, when an error is encountered.
To use this method the transition need to be modified to store the error, see
\cref{fig:error1}. The advantage of this is that when a lexical error is
encountered nothing more will be computed for that transition, however all other
transitions in the transition map will be computed as normal. If this style is
used in a text editor and a lexical error is encountered, the user will only get
feedback from that error.

\begin{figure}[!ht]
\begin{lstlisting}[language=Haskell]
type Transition = (State,Either ([Token],State) Error)
\end{lstlisting}
\caption{Transition that can either contain tokens or a lexical error\label{fig:error1}}
\end{figure}

Another way is to keep as much of the correct tokens as possible and only store
errors for the lexeme that does not match anything else. With this approach the
lexer would store all tokens up until a lexical error is encountered. When an
error is encountered, the error is stored and the lexer tries to lex the rest of
the text starting from the starting state. For this to work the sequence that
stores the tokens needs to store the lexical errors aswell, see
\cref{fig:error2}. With this approach the lexer will continue combining tokens
after a lexical error is found, the drawback with this is that extra
token computations needs to be made that may not be usefull in the final lexical
analysis. If this approach is used in a text editor the user will see the minimal
combination of characters that construct a lexical error. After that error,
tokens that are lexed from the starting state is returned.

\begin{figure}[!ht]
  \begin{lstlisting}[language=Haskell]
    type Transition = (State,[Either Token Error],State)
  \end{lstlisting}
  \caption{Transition contains a sequence of tokens and errors\label{fig:error2}}
\end{figure}

\newpage

\begin{example}[A java lette light lexer, see \cref{reglang}]
Lexical analysis is done on the string ``Hello /*World''. When global error
handling, the transition contains one error or a sequence of tokens, is used the
result of the lexical analysis will be as in \cref{fig:globerr}. When local
error handling, the transition contains a sequence of tokens and errors, is used
the result of the lexical analysis will be as in \cref{fig:locaerr}.

\begin{figure}[!ht]
  \centering
  \subfigure[Global Error]{
    \begin{tabular}{ll}
      String & Type\\
      '/' & $Error$\\
      & \\
    \end{tabular}
    \vspace{4ex}
    \label{fig:globerr}
  }
  \hspace{2cm}
  \subfigure[local Error]{
    \begin{tabular}{ll}
      String & Type\\
      'Hello' & $Ident$\\
      ' ' & $Space$\\
      '/' & $Error$\\
      '*' & $Reserved$\\
      'World' & $Ident$\\
       & \\
    \end{tabular}
    \vspace{4ex}
    \label{fig:locaerr}
  }
  \caption{Difference in error handling}
\end{figure}

If local error handling is used and the comment in the string would later on be
closed, the tokens after '/' would be thrown away since another transition would
be used which constructs a multi line comment.
If global error handling is used the user will get little to no use of the
lexical analysis until the lexical error is corrected, however run time is saved
since nothing is computed after the lexical error is found for that transition.
\end{example}

\subsection{Expected Time Complexity}
Incremental computing states that only content which depends on the new
data will be recalculated. That is, follow the branch of the tree from the new
leaf to the root and recalculate every node on this path. As shown by
\cref{fig:incUp}. Only one subproblem is updated in every level of the tree.
Using the master method to calculate the expected time complexity gives:
$e = \log_b a$ where $a$ is number of recursive calls and $n/b$ is size of the
subproblem where $n$ is the size of the original problem. As shown by
\cref{fig:incUp}, the number of needed update calls at every level of the tree
is $1$, therefore $a = 1$. The constant $b$ is still $2$. This will give
$e = \log_2 1 = 0$. Thus the update function of the incremental algorithm will
have an expected time complexity of $\Theta(n^0 \cdot \log n) = \Theta(\log n)$

Since the fingertree is lazy, when an element is added to the root level of the
tree, root elements might be pushed down in the tree. The measure of the
lower levels does not need to be immediately recalculated. Instead they
are recalculated when they are used. Paying for this expensive operation like
described in the section about bankers method \cite{fingertree}.
