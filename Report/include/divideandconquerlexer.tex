\chapter{Divide-and-Conquer Lexer}
An incremental divide and conquer lexer works by dividing the sequence, to be
lexically analyzed, into small parts and analyzes them; and then combining them.
In the base case the lexical analysis is done on a single character. The
conquer step then combines the smaller tokens into as large tokens as possible.
The end result is a sequence of token that represent the code. How this is done
is described in this chapter.

\section{Divide and Conquer in General}
This section gives an idea of how the Divide and Conquer algorithm
works in general, before addressing in detail how to apply it to lexing. It
describes the power of divide and conquer in terms of executing time and how
laziness can be applied to these algorithms.

\subsection{The Three Steps}
The general idea of a divide and conquer algorithm is to divide a problem into
smaller parts, solve them independently and then combine the results. A Divide
and Conquer algorithm always consists of a pattern with these three steps
\cite{Goodrich}.
\begin{description}
\item[Divide:] If the input size is bigger than the base case then divide the
input into subproblems. Otherwise solve the problem using a straightforward
method.
\item[Recur:] Recursively solve the subproblems associated with the subset.
\item[Conquer:] Given the solutions to the subproblems, combine the results to
solve the original problem.
\end{description}

\subsection{Associative Function}
An associative function, or operator, is a function that does not care in what
order it is applied. An example of such a function is addition $(+)$ of numbers,
which is associative since it has the property in \cref{assprop}.

In divide and conquer algorithms this is essential. In the divide step of the
divide and conquer algorithm, there is no certain order of how the subproblems
are going to be divided. This means that the order the subproblems are being
conquered can not have an impact on the algorithm, hence the conquer step must be
associative.

\begin{example}[Associativity of the conquer step]\label{assprop}
Let $f(x,y)$ be the conquer function, where $x$ and $y$ are of the same type as
the result of $f$, then:
\begin{center}
$f(x,f(y,z)) = f(f(x,y),z)$
\end{center}
Otherwise the algorithm can give different results for the same data.
\end{example}

\subsection{Time Complexity}
To calculate the running time of any divide and conquer algorithm the master
method can be applied \cite{Cormen}. This method is based on the following
theorem.
\begin{theorem}[Master Theorem \cite{Cormen} \label{MasterTheo}] $ $\\
Assume a function $T_n$ constrained by the recurrence
\begin{center}
$T_n = {\alpha}T_{\frac{n}{\beta}}+ f(n)$
\end{center}
(This is typically the equation for the running time of a divide and conquer
algorithm, where $\alpha$ is the number of subproblems at each recursive step,
$n/\beta$ is the size of each subproblem, and $f(n)$ is the running time of
dividing up the problem space into $\alpha$ parts, and combining the results
of the subproblems together.)\\
If we let $e = \log_\beta \alpha$, then
\begin{center}
\begin{tabular}{r c l l}
1. $T_n$ & $=$ & $\Theta(n^{e})$ &  if $f(n) = O(n^{e - \epsilon})$ and $\epsilon > 0$\\
2. $T_n$ & $=$ & $\Theta(n^{e} \log n)$ & if $f(n) = \Theta(n^e)$\\
3. $T_n$ & $=$ & $\Theta(f(n))$ & \begin{minipage}[t]{0.6 \columnwidth}
  if $f(n) = \Omega(n^{e+\epsilon})$ and $\epsilon > 0$
  and $\alpha \cdot f(n/\beta) \leq c \cdot f(n)$
  where $c < 1$ and all sufficiently large $n$
  \end{minipage}
\end{tabular}
\end{center}
\qeda
\end{theorem}

\subsection{Hands on Example}
The divide and conquer pattern can be performed on algorithm that solves
different problems. A general problem is sorting, or more precisely sorting a
sequence of integers. This example shows merge-sort.

\begin{description}
\item[Divide:] The algorithm starts with the divide step. Given the input $S$
the algorithm will check if the length of $S$ is less then or equal to 1.
\begin{itemize}
\item If this is true, the sequence is returned. A sequence of one or zero
elements is always sorted.
\item If this is false, the sequence is split into two equally big sequences,
$S_1$ and $S_2$. $S_1$ will be the first half of $S$ while $S_2$ will be the
second half.
\end{itemize}
\item[Recur:] The next step is to sort the subsequences $S_1$ and $S_2$. The
sorting function sorts the subsequences by recursively calling itself twice with
$S_1$ and $S_2$ as arguments respectively.
\item[Conquer:] Since $S_1$ and $S_2$ are sorted combining them into one sorted
sequence is trivial. This process is what is referred to as merge in merge-sort.
The resulting sequence of the merge is returned.
\end{description}
Algorithm~\ref{Alg:MergSort} shows a more formal definition of merge-sort.

\begin{algorithm}
\DontPrintSemicolon
\KwData{Sequence of integers $S$ containing $n$ integers}
\KwResult{Sorted sequence $S$}
\If {$length(S) \leq 1$}{
  \Return $S$ \;
}
\Else {
  $(S_1,S_2) \gets splitAt(S,n/2)$ \;
  $S_1 \gets MergeSort(S_1)$\;
  $S_2 \gets MergeSort(S_2)$\;
  $S \gets Merge(S_1, S_2)$\;
  \Return $S$
}
\caption{MergeSort}
\label{Alg:MergSort}
\end{algorithm}

Given the merge-sort algorithm, time complexity can be calculated as follows
using the master method. There are $2$ recursive calls and the subproblems are
$1/2$ of the original problem size, so $\alpha=2$ and $\beta=2$. To merge the
two sorted subproblems the worst case is to check every element in the two list,
$f(n) = 2 \cdot n/2 = n$.
\begin{center}
$T(n) = 2T(n/2) + n$\\
$e=\log_\beta\alpha=\log_2 2=1$
\end{center}
Case 2 of the \cref{MasterTheo} applies, since
\begin{center}
$f(n) = \Theta(n)$
\end{center}
So the solution will be:
\begin{center}
$T(n) = \Theta(n^{\log_2 2} \cdot \log n) = \Theta(n \cdot \log n)$
\end{center}

\subsection{Incremental Computing}
To be incremental means that, whenever some part of the data to the algorithm
changes the algorithm tries to save time by only recomputing the changed data
and the parts that depend on this changed data \cite{incrementalDef}. This is
illustrated in \cref{fig:incUp}.

\begin{figure}[!htp]
\centering
  \begin{tikzpicture}[level/.style={sibling distance=60mm/#1, align=center, text centered}]
\node [leaf,red] (z){$r'$}
  child { node[leaf,red] (a) {$c'$}
    child {node[leaf] (b) {$a$}
      child {node[leaf] (c) {$-$}} 
      child {node[leaf] (d) {$-$}}
    }
    child {node[leaf,red] (e) {$b'$}
      child {node[leaf] (f) {$x$}}
      child {node[leaf,red] (g) {$y'$}}
    }
  }
  child {node[leaf] (i) {$d$}
    child {node[leaf] (j) {$-$}
      child {node[leaf] (k) {$-$}}
      child {node[leaf] (l) {$-$}}
    }
    child {node[leaf] (m) {$-$}
      child {node[leaf] (n) {$-$}}
      child {node[leaf] (o) {$-$}}
    }
  };
\end{tikzpicture}
\caption{When the node $y$ changed recomputed nodes are marked with a $'$. \label{fig:incUp}}
\end{figure}

For a divide and conquer lexer this means to only recompute the changed token
and the token to the right of the changed token. This is done recursively until
the root of the tree is reached. The expected result of this would be that when
a character is added to the code of 1024 tokens, instead of recalculate all the 
1024 tokens the lexer only needs to do 10 recalculations. Since,
$log_2 1024 = 10$. This can be explained by the \cref{MasterTheo}.

Only one branch in the tree will be followed at every level and the problem is
already divided. Therefore the parameters will be set to:
\begin{center}
$\alpha = 1$, $\beta = 2$ and $f(n) = 1$.
$e=log_\beta\alpha=\log_2 1=0$
\end{center}
Case 2 of the \cref{MasterTheo} applies, since
\begin{center}
$f(n) = \Theta(n^e)$
\end{center}
The complexity is therefore:
\begin{center}
$T(n) = \Theta(n^e \cdot \log n) = \Theta(\log n)$
\end{center}

\section{Fingertree}
Fingertree is a tree structure which is incremental in its nature and has good
performance. To ensure that an incremental divide and conquer algorithm can
access the intermediate states a data structure like fingertrees can be used.
Before describing how the fingertree is defined, an introduction to the
fingertrees building blocks is given \cite{fingertree}.

\subsection{Fundamental Concepts}

Fingertrees uses monoids which in abstract algebra is a set, $S$, and a binary
operation $\bullet$ which fulfills the following
three properties:
\begin{description}
\item[Closure] $\forall a,b \in S: a \bullet b \in S$
\item[Associativity] $\forall a,b,c \in S: (a \bullet b) \bullet c = a \bullet
    (b \bullet c)$ 
\item[Identity element] $\exists e \in S: \forall a \in S: e \bullet a = a
    \bullet e = a$
\end{description}

Fingertrees uses Right and Left Reductions. This is a function which
collapses a structure of $f$ $a$ into a single value of type $a$. The base case
for when the tree is empty is replaced with a constant value, such as 
$\emptyset$. Intermediate results are combined using a binary operation, like
$\bullet$. Reduction with a monoid always returns the same value,
independently of the argument nesting. For a reduction with an arbitrary
constant and a binary operation there must be a specified nesting rule. If
combining operations are only nested to the right, or to the left, the obtained
result will be a skewed reductions, which can be singled out as a type class
described in \cref{fig:Reduction} \cite{fingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeReduceFun.hs}
\caption{Reduction function in Haskell \label{fig:Reduction}}
\end{figure}

In the case for lists, left and right reductions are equivalent to foldl and foldr.

\subsection{Simple Sequence}
The fingertrees can be described by comparing it to an already
known data structure and how that data structure represent data. Lets take a look
at the definition on a 2-3 fingertree and how they can implement a sequence.
Lets start by looking at an ordinary 2-3 tree representing the string "thisisnotatree".

\begin{figure}[!h]
\centering
\begin{tikzpicture}[auto, level 1/.style={sibling distance=7.5cm},
  level 2/.style={sibling distance=2.5cm},
  level 3/.style={sibling distance=1cm},
  level distance = 0.5cm]
    \node [branch] {}
        child{ node [branch] {}
            child{ node [branch] {}
            	child{ node [leaf] {t}
                }
			    child{ node [leaf] {h}
                }
            }
            child{ node [branch] {}
            	child{ node [leaf] {i}
                }
			    child{ node [leaf] {s}
                }
            }
            child{ node [branch] {}
            	child{ node [leaf] {i}
                }
			    child{ node [leaf] {s}
                }
            }
        }
        child{ node [branch] {}
            child{ node [branch] {}
            	child{ node [leaf] {n}
                }
			    child{ node [leaf] {o}
                }
			    child{ node [leaf] {t}
                }
            }
            child{ node [branch] {}
            	child{ node [leaf] {a}
                }
			    child{ node [leaf] {t}
                }
            }
            child{ node [branch] {}
            	child{ node [leaf] {r}
                }
			    child{ node [leaf] {e}
                }
			    child{ node [leaf] {e}
                }
            }
        }
    ; 
\end{tikzpicture}
\caption{Ordinary 2-3 tree
\label{fig:2-3tree}}
\end{figure}

The tree shown in the \cref{fig:2-3tree} stores all it is data in the leafs.
This can be expressed by defining an non-regular or nested type, as shown in
\cref{fig:2-3Fingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTree2-3Tree.hs}
\caption{Definition of a 2-3 Fingertree \label{fig:2-3Fingertree}}
\end{figure}

Operations on these types of trees usually takes logarithmic time in the size of
the tree. However in a sequence representation, constant time complexity is
preferable for adding or removing element from the start or end of the sequence.

A finger is a structure which provides efficient access to nodes near the
distinguished location. To obtain efficient access to the starting and ending
elements of the sequence represented by the tree, there should be fingers placed
at these positions of the tree. In the example tree, taking hold of the end and start
nodes of and lifting them up together. The result should look like in
\cref{fig:fingertree}

\begin{figure}[!h]
\centering
\begin{tikzpicture}[level distance=2cm, sibling distance = 2cm]
    \node[blackbranch] {}
        child { node[leaf] {t} }
        child { node[leaf] {h} }
        child[level distance=2cm, sibling distance=2.5cm, grow=down]
          { node[blackbranch] {}[level distance=2cm]
            child { node[branch] {} [sibling distance=1cm]
                child { node[leaf] {i} }
                child { node[leaf] {s} }
            }
            child { node[branch] {} [sibling distance=1cm]
                child { node[leaf] {i} }
                child { node[leaf] {s} }
            }
            child[level distance=2cm, grow=down]
              { node[blackbranch, fill=white] {}}
            child { node[branch] {} [sibling distance=1cm]
                child { node[leaf] {n} }
                child { node[leaf] {o} }
                child { node[leaf] {t} }
            }
            child { node[branch] {} [sibling distance=1cm]
                child { node[leaf] {a} }
                child { node[leaf] {t} }
            }
        }
        child { node[leaf] {r} }
        child { node[leaf] {e} }
        child { node[leaf] {e} }
    ;
\end{tikzpicture} 
\caption{2-3 Fingertree
\label{fig:fingertree}}
\end{figure}

Since all leafs in the 2-3 tree are at the same level, the left and right
spine has the same length. Therefore the left and right spines can be paired up
to create a single central spine. Branching out from the spine are 2-3 trees. At
the top level there are two to three elements on each side, while the other
levels have one or two subtrees, whose depth increases down the spine.
Depending on if the root node had 2 or 3 branches in the original 2-3 tree, The
bottom node will have either a single 2-3 tree or an empty tree. This structure
can be described as shown in \cref{fig:DataTypeFingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/HaskellFingerTree.hs}
\caption{Definition of the Fingertree data type \label{fig:DataTypeFingertree}}
\end{figure}

Where Digit is a buffer of elements stored left to right, here represented as a
list for simplicity

The non-regular definition of the $FingerTree$ type determines the unusual shape
of these trees, which is the key to their performance. The top level of the tree
contains elements of type $a$. Next level contains elements of type $Node$ $a$.
At the $n$th level, elements are of type $Node^n$ $a$. which are 2-3 trees with
a depth of $n$. This gives that a sequence of $n$ elements is represented by
a $FingerTree$ of depth $\Theta(\log n)$. An element at position $d$ from
the nearest end is stored at a depth of $\Theta(\log d)$ in the $FingerTree$

In fingertrees and nodes the reduce function mentioned in fundamental concepts
is genericly defined to the following types. 
Reduction for the node which is shown in \cref{fig:reductionNode}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeReduceNode.hs}
\caption{Reduction of a fingertrees node \label{fig:reductionNode}}
\end{figure}

For the fingertrees reduction instance both single and double lifting of the binary
operation is used as shown in \cref{fig:reductionFingerTree} \cite{fingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeReduceFingerTree.hs}
\caption{Reduction of a Fingertree \label{fig:reductionFingerTree}}
\end{figure}

\subsection{Double-ended Queue Operations}
After showing how the Fingertrees basic structure is defined, lets take a look
on how fingertrees makes efficient Double-ended Queue, a queue which can be
accessed from both ends, where both the operations having the time complexity
$\Theta(1)$.

Adding an element to the beginning of the sequence is trivial, except
when the initial buffer ($Digit$) already is full. In this case, push all but
one of the elements in the buffer as a node, leaving behind two elements in the
buffer, shown in \cref{fig:AddFirst}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeInfixr.hs}
\caption{Adding an element to the beginning of the sequence \label{fig:AddFirst}}
\end{figure}

Adding to the end of the sequence is a mirror image of the code in
\cref{fig:AddFirst} and is shown in \cref{fig:AddLast}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeInfixl.hs}
\caption{Adding an element to the end of the sequence \label{fig:AddLast}}
\end{figure}

In the basic 2-3 tree, where the data is stored in the leaves, an insertion
operation is done with a time complexity of $\Theta (\log n)$. The expected 
time complexity of a fingertree can be expressed in this way:
Digits of two or three elements (which is being of similar structure to elements
of type $Node$ $a$) are classified as safe since these can easily be mapped to a
$Node$ and those of one or four elements are classified as dangerous. A
double-ended queue operation can only propagate to the next level from a
dangerous element. By doing so making that dangerous element safe, which means
that the next operation reaching that digit will not propagate. This will result
in that at most $1/2$ of the operations descend one level, at most $1/4$ two
levels, and so on. This will give that in a over time the average cost of
operations is constant.

The same bounds hold in a persistent setting if subtrees are suspended using lazy
evaluation. Laziness makes sure that changes deep in the spine do not take place
until a subsequent operation need to go that far. From the above properties of
safe and dangerous digits, by the time a change deep down in the tree is needed
enough cheap shallow operations will have been performed to pay for the more
expensive operation. This can be expressed more in detail by the bankers method
\cite{fingertree}.

\subsubsection{The Bankers Method}
The bankers method is a technique used to calculate the practical time
assumption where it accounts for accumulated debt. A form of currency called
debit is used. Where each debit correspond to an constant amount of suspended
work. When a computation initially suspends, it create a number of debits
proportional to it is shared cost and associate each debit with a location
in the object. The selection of location for every debit depends on the type
of the computation. If the computation is monolithic (i.e., once begun, it
runs to completion), then all debits are usually assigned to the root of the
result, which the fingertree is not. But if the computation is lazy, like for
the fingertree, then the debits may be distributed among the roots of the partial
results.

The amortized cost of an operation is the unshared cost of the operation
plus the number of debits discharged by the operation. Where an unshared cost
for an operation does not include the number of debits created by an operation.
The ordering of how debits should be discharged depends on the probable ordering
of accesses to objects; debits on nodes with the highest likelihood to be accessed
first should also be discharged first.

Incremental functions play an important part in the bankers method since
they allow debits to be dispersed to different locations in a data structure,
each corresponding to a nested suspension. This means that each location can be
accessed as soon as its debits are discharged, without waiting for other
locations debits to be discharged. This results in that the initial partial
results of an incremental computation can be paid for quickly, and that
subsequent partial results may be paid for as they are needed \cite{Okasaki1999}.

\subsubsection{Banker Method on the Fingertree}
The amortized time can be argued for by using the Banker method.
This is done by assigning the suspension of the middle tree in each Deep node
as many debits as the node has safe digits. (0,1 or 2) A double-ended queue
operation which descends $k$ levels turns $k$ dangerous digits into safe digits.
By doing so creates $k$ debits to pay for the work done.
Applying the bankers method of debit passing to any debits already attached to
these $k$ nodes. It can be shown that each operation must discharge at most
one debit. Therefore the double-ended queue operations run in $\Theta(1)$
amortized time \cite{fingertree}.


\subsection{Concatenation Operations}
Concatenation is a simple operation for most cases, except for the case when two
$Deep$ trees are being concatenated. Since $Empty$ is the identity element,
concatenation with an $Empty$ yields the other tree. Concatenation with a
$Single$ will reduce to $<|$ or $|>$. For the hard part when there are two
$Deep$ trees, the prefix of the first tree will be the final prefix. Suffix of
the second tree will be the suffix of the final tree. The recursive function
$app3$ shown in \cref{fig:concatHelp} combines two trees and a list of $Nodes$
(basically the old prefix and suffixes down the spines of the old trees).

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeApp3.hs}
\caption{Help function for concatenating two fingertrees \label{fig:concatHelp}}
\end{figure}

Where $(<|')$ and $(|>')$ are the functions defined in \cref{fig:reduceAppend}
and $nodes$ groups a list of elements into $Node$s as shown in
\cref{fig:nodesHelp}. 

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/ReduceAppend.hs}
\caption{Help function for transforming a list of element into a list of Nodes \label{fig:reduceAppend}}
\end{figure}


\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeNodesFunc.hs}
\caption{Help function for transforming a list of element into a list of Nodes \label{fig:nodesHelp}}
\end{figure}

The concatenation of the Fingertrees calls on $app3$ with an empty list
between the two trees, as shown in \cref{fig:concat}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell]{examples/FingerTreeConcatFunc.hs}
\caption{Concatenation function for Fingertree \label{fig:concat}}
\end{figure}

The time spent on concatenation can be reasoned in this way. Each invocation of
$app3$ arising from $(><)$ the argument list has a length of at most 4, which
means that each of these invocations takes $\Theta(1)$ time. The recursion
terminates when the bottom of the shallower tree has been reached, with up to
4 insertions. So the total time complexity is $\Theta(\log min\{n_1, n_2\})$
where $n_1$ and $n_2$ are the number of elements in the two trees
\cite{fingertree}.

\subsection{Measurements}
Fingertrees has been shown to work well as catenable double-ended queues. A 
measurement is a property describing the state of the tree. This section present
Paterson and Hinze modification of the fingertree, which provides positional and
set-theoretic operations. For example taking or dropping the first $n$ elements.
These operations involve searching for an element with a certain property.
To implement additional operations with good performance there must be a way
to steer this search. A way to measure the tree.

A measurement can be viewed as a cached reduction with some monoid. 
Reductions, possibly cached, are captured by the class declaration in 
\cref{fig:measure}. Where $a$ is the type of a tree and $v$ the type of an
associated measurement. $v$ must be of a monoidal structure so measurements of
subtrees easily can be combined independently of the nesting. Take the size of a
tree as an example. Measure maps onto the monoid over the set of natural numbers
and with the binary operator of addition \cite{fingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell, mathescape=true]{examples/ClassMeasure.hs}
\caption{Definition of the Measure class \label{fig:measure}}
\end{figure}

\subsubsection{Caching measurements}
It should be cheap to obtain a measurement. The fingertree should ensure that an
measurement can be obtain with a bounded number of $\bullet$ operations.
Therefore fingertrees cache the measurements in the 2-3 nodes. In
\cref{fig:measureNode} the Measure of Node is shown.
The constructors and the instance declaration are completely generic: they
work for arbitrary annotations.

\begin{figure}[h!]
\lstinputlisting[language=Haskell, mathescape=true]{examples/Node2-3Measure.hs}
\caption{Measure of the data type Node \label{fig:measureNode}}
\end{figure}

Digits are measured on the fly. As the length of the buffer Digit is bounded by
a constant, the number of $\bullet$ operations is also bounded.

\begin{figure}[h!]
\lstinputlisting[language=Haskell, mathescape=true]{examples/DigitMeasure.hs}
\caption{Measure of the data type Digit \label{fig:measureDigit}}
\end{figure}

Fingertrees are modified in a similar manner to 2-3 nodes.
The top level of a measured fingertree contains elements of type $a$, the second
level of type $Node$ $v$ $a$, the third of type $Node$ $v$ ($Node$ $v$ $a$), and
so on. The Measure function is shown in \cref{fig:measureFingerTree}. The tree 
type $a$ changes from level to level, whereas the measure type $v$ remains the 
same. This means that Fingertree is nested in $a$, but regular in $v$ \cite{fingertree}.

\begin{figure}[h!]
\lstinputlisting[language=Haskell, mathescape=true]{examples/FingerTreeMeasure.hs}
\caption{Fingertrees Measure function \label{fig:measureFingerTree}}
\end{figure}


\subsection{Sequences}
A sequence in Haskell is a special case of the fingertree that has no measure.
The performance is therefore superior to that of standard lists. Where a list in
Haskell has $\Theta(n)$ for finding, inserting or deleting elements, that is in
a list there is only known current element and the rest of the list. Results in 
finding the last element of a list, the computer must look at every element
until the empty list has been found as the rest of the list. Where in a sequence
the last element can be obtained in $\Theta(1)$ time. Adding an element anywhere
in the sequence can be done in worst case, $\Theta(log n)$ \cite{fingertree}.

\section{Divide and Conquer Lexing in General}
In the last section we covered the general divide and conquer algorithm. This
section covers the general data structures and algorithms for an incremental
divide and conquer lexer.

\subsection{Tree structure}
The incremental divide and conquer lexer should use a structure where the
code-lexemes can be related to its tokens, current result can be saved and
easily recalculated. A divide and conquer lexer should therefore use a tree
structure to save the lexed result in. Since every problem can be divided into
several subproblems, until the base case is reached. This is cleraly a tree
structure of solutions, where a leaf is a token for a single character, and the
root is a sequence of all tokens in the code.  

\subsection{Transition map}
When storing a result of a lexed string it is a good idea to store more then
just the tokens. In particular the in and out states are needed when combining
the lexed string with another string. We will henceforth refer to this as a
$transition$.
\begin{lstlisting}[language=Haskell]
type Transition = (State,[Token],State)
\end{lstlisting}
Since the lexer does not know if the current string is a prefix of the entire
code or not it can not make any assumptions on the in state. Because of this the
lexer needs to store a transition for every possible in state, we will henceforth
refer to this as a \emph{transition map}.
\begin{lstlisting}[language=Haskell]
type Transition_map = [Transition]
\end{lstlisting}
\subsubsection{The Base Case}
When the lexer tries to lex one character it will create a transition
map using the DFA for the language. It will for each state create a transition
that has the state as in state, a list containing the character as the only
token and by using the DFA, lookup what out state the transition should have.
For the character '/' part of a transition map might look like \cref{fig:bascas}.

In \cref{fig:bascas}, \cref{fig:conq} and \cref{fig:longconq} the first number
refers to the in state, the middle part is the sequence of tokens and the second
number is the out state, that can be accepting.

\begin{figure}[!ht]
\begin{center}
$\left[\begin{array}{ccc}
0&['o']&Accepting 5\\
1&['o']&1\\
10& &NoState\\
\end{array}\right]$
\caption{The Base Case for divide and conquer lexing \label{fig:bascas}}
\end{center}
\end{figure}

$NoState$ transition is used to tell the lexer that using that particular 
transition will result in a lexical error. For reasons being covered later in
this section, they can not be discarded.
%\cref{longmatch}

\subsubsection{Conquer Step}
The conquer step of the algorithm is to combine two transition maps into one
transition map. This is done by, for every transition in the left transition map, combining
the transition with the transition in the right transition map that has the same in state as the
left transitions out state. This can be described by the following logical statement where $T_1$ and $T_2$ refers to the first and second transition map.
\begin{center}
$\forall.t_1\in T_1\;\exists.t_2\in T_2\;o_1=i_2,o_1=outState(t_1),i_2=inState(t_2)
\vdash t_{new}=merge(t_1,t_2)$
\end{center}

The most general case is a naive lexer that takes the first accepting state it
can find. When two transitions are combined there are two different outcomes:
\begin{description} 
  \item[Append:]If the out state of the first transition is accepting, the
    sequence in the transition that starts in the starting state of the second
    transition map will be appended to the first.
\begin{lstlisting}[language=Haskell, mathescape=true]
appendTokens :: Tokens -> Tokens -> Tokens 
appendTokens tokens1 tokens2 = tokens1 >< tokens2
\end{lstlisting}

%create two tokens if the
%    out state of the first list is accepting then the second list will be appended
%    to the first list.
  \item[Merge:]If the out state of the first transition is not accepting, the
    transition in the second transition map with the same in state as the out
    state of the first transition will be used. The last token of the sequence
    from the first transition will be merged with the first token in the second
    transition into one token and put between the two sequences.\\

\begin{lstlisting}[language=Haskell, mathescape=true]
mergeTokens :: Tokens -> Tokens -> Tokens 
mergeTokens tokens1 tokens2 = prefix1 |> newToken >< suffix2
  where prefix1 |> token1'  = tokens1
        tokens2' <| suffix2 = tokens2
        newToken            = token1' `combinedWith` tokens2'
\end{lstlisting}

%The other case is when the first list of tokens does not end
%    in an accepting state. In this case the lexer will try to find an in state in
%    the second list that is the same as the out state of the first transition.
\end{description}
For both the cases the in state of the first transition will be the new in state
and the out state of the second transition will be the new out state. An example
of bothe cases is shown in \cref{fig:conq}.

\begin{figure}[!ht]
\begin{center}
$\left[\begin{array}{ccc}
0&['o']&Accepting 5\\
1&['o']&1\\
\end{array}\right] `combineTokens`
\left[\begin{array}{ccc}
0&['~~']&Accepting 2\\
1&['~~']&1\\
\end{array}\right] =
\left[\begin{array}{ccc}
0&['o','~~']&Accepting 2\\
1&['o~~']&1\\
\end{array}\right]$
\caption{The Conquer step for Divide and Conquer lexing\label{fig:conq}}
\end{center}
\end{figure}

This will not work as a lexer for most languages since the longest match rule is
not implemented. For example, it will lex a variable to variables where the
length is a single character, for example ``os'' will be lexed as two tokens,
``o'' and ``s''. To solve this some more work is needed.

\subsubsection{Longest Match}\label{longmatch}
To ensure that only the longest token is returned some stricter rules for
combinations are needed. Firstly, if two transitions can be combined without
having the outgoing state $NoState$ then merge those transition. When two
transitions are merged the last token of the left transition is merged with
the first token of the right transition into one token. Secondly, If the
combination of two transtitions would yield $NoState$, the transitions are
appended instead. When two transitions are appended the right transition
starting from the starting state is appended to the left transition. As can be
seen in \cref{fig:longconq} 

\begin{figure}[!ht]
\begin{center}
$\left[\begin{array}{ccc}
0&['o','~~']& Accepting 2\\
1&['o~~'] & 1\\
\end{array}\right] `combineTokens` 
\left[\begin{array}{ccc}
0&['/','*']&Accepting 4\\
1&['*/']&Accepting 3\\
2& &NoState\\
\end{array}\right] =
\left[\begin{array}{ccc}
0&['o','~~','*','/']&Accepting 4\\
1&['o~~*/']& Accepting 3\\
\end{array}\right]$
\caption{The Conquer step when the longest match rule is applied\label{fig:longconq}}
\end{center}
\end{figure}

When two transitions are appened another rule needs to be accounted for. If the
last token of the first transition does not end in an accepting state a lexical
error is found. How lexical errors are handled and stored is explained in
\cref{sec:lexerr}.

\subsection{Lexical Errors}\label{sec:lexerr}
Even though lexical errors can not halt the lexer it is still usefull to keep
them since they tell the user what is wrong. In an incremental lexer there are
different ways this can be achieved. The simplest way is to store the lexical
error, instead of the tokens and outgoing state, when an error is encountered.
To use this method the transition need to be modified to store the error, see
\cref{fig:error1}. The advantage of this is that when a lexical error is
encountered nothing more will be computed for that transition, however all other
transitions in the transition map will be computed as normal. If this style is
used in a text editor and a lexical error is encountered, the user will only get
feedback from that error.

\begin{figure}[!ht]
\begin{lstlisting}[language=Haskell]
type Transition = (State,Either ([Token],State) Error)
\end{lstlisting}
\caption{Transition that can either contain tokens or a lexical error\label{fig:error1}}
\end{figure}

Another way is to keep as much of the correct tokens as possible and only store
errors for the lexeme that does not match anything else. With this approach the
lexer would store all tokens up until a lexical error is encountered. When an
error is encountered, the error is stored and the lexer tries to lex the rest of
the text starting from the starting state. For this to work the sequence that
stores the tokens needs to store the lexical errors aswell, see
\cref{fig:error2}. With this approach the lexer will continue combining tokens
after a lexical error is found, the drawback with this is that extra
token computations needs to be made that may not be usefull in the final lexical
analysis. If this approach is used in a text editor the user will see the minimal
combination of characters that construct a lexical error. After that error,
tokens that are lexed from the starting state is returned.

\begin{figure}[!ht]
  \begin{lstlisting}[language=Haskell]
    type Transition = (State,[Either Token Error],State)
  \end{lstlisting}
  \caption{Transition contains a sequence of tokens and errors\label{fig:error2}}
\end{figure}

\begin{example}[A java lette light lexer \cref{reglang}]
Lexical analysis is done on the string ``Hello /*World''. When global error
handling, the transition contains one error or a sequence of tokens, is used the
result of the lexical analysis will be as in \cref{fig:globerr}. When local
error handling, the transition contains a sequence of tokens and errors, is used
the result of the lexical analysis will be as in \cref{fig:locaerr}.

\begin{figure}[!ht]
  \centering
  \subfigure[Global Error]{
    \begin{tabular}{ll}
      String & Type\\
      '/' & $Error$\\
    \end{tabular}
    \label{fig:globerr}
  }
  \subfigure[local Error]{
    \begin{tabular}{ll}
      String & Type\\
      'Hello' & $Ident$\\
      ' ' & $Space$\\
      '/' & $Error$\\
      '*' & $Reserved$\\
      'World' & $Ident$\\
    \end{tabular}
    \label{fig:locaerr}
  }
  \caption{Difference in error handling}
\end{figure}

If local error handling is used and the comment in the string would later on be
closed, the tokens after '/' would be thrown away since another transition would
be used which constructs a multi line comment.
If global error handling is used the user will get little to no use of the
lexical analysis until the lexical error is corrected, however run time is saved
since nothing is computed after the lexical error is found for that transition.
\end{example}

\subsection{Expected Time Complexity}
Incremental computing states that only content which depends on the new
data will be recalculated. That is, follow the branch of the tree from the new
leaf to the root and recalculate every node on this path. As shown by
\cref{fig:incUp}. Only one subproblem is updated in every level of the tree.
Using the master method to calculate the expected time complexity gives:
$e = log_b a$ where $a$ is number of recursive calls and $n/b$ is size of the
subproblem where $n$ is the size of the original problem. As shown by
\cref{fig:incUp}, the number of needed update calls at every level of the tree
is $1$, therefore $a = 1$. The constant $b$ is still $2$. This will give
$e = log_2 1 = 0$. Thus the update function of the incremental algorithm will
have an expected time complexity of $\Theta(n^0 \cdot log n) = \Theta(log n)$

Since the fingertree is lazy, when an element is added to the root level of the
tree, root elements might be pushed down in the tree. The measure of the
lower levels does not need to be immediately recalculated. Instead they
are recalculated when they are used. Paying for this expensive operation like
described in the section about bankers method \cite{fingertree}.
