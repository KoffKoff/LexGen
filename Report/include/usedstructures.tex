\chapter{Implementation}
In this chapter the tools, data structure and implementation of the incremental
divide and conquer lexer is explained. The implementation can be found in
\cref{chap:code}.

\section{Alex}
Alex is a tool for generating lexical analyzers built in Haskell given a
description of the language in the form of regular expressions, it is similar to
lex and flex in C and C++. The resulting lexer is Haskell 98 compatible and can
easily be used with the parser Happy, a parser generator for Haskell\cite{alex}.
Alex is notably used in BNFC which is a program to generate among other things a
lexer, parser and abstract syntax from Backus-Naur Form. The modules generated
by BNFC can easily be used to create a compiler or interpreter\cite{bnfc}.

\subsection{The DFA design}
In the incremental lexer Alex was used to create the DFA. The reason for this is
that Alex generates a DFA which is optimized in data size. Instead of having an
array for every possible character and state combination 5 arrays are generated
that takes advantage of the fact that for most characters the same state will be
used in the majority of time, thus saving a lot of elements that would
otherwise be the same in the array.

The trade of for using the Alex generated DFA is that some minor arithmetic
operations are used and some extra lookups are needed. These operations
are far less time consuming then the rest of the lexical operations.

\section{Token data structure}
To keep all the information that might be needed when combining two texts, a data
structure for the tokens was created. This data structure contains slightly more
data then what a sequential lexer would save.

\subsection{Sequences}
Since this project is about creating a real-time lexing tool, performance is
important. Therefor there are advantages of using sequences instead of lists.
The most notable place where this is used is in the measure of the fingertree,
where the tokens are stored in a sequence rather then a list. Sequences are also
used elsewhere in the project but this is the most notable place since it is
frequently updated.

\subsection{Tokens}
The internal structure used to store lexed tokens is called $Tokens$. There are
three constructors in the $Tokens$ data type.
\lstinputlisting[language=Haskell]{examples/Token.hs}
$NoTokens$ is a representation of when an empty string has been lexed.
$InvalidTokens$ represents a lexical error somewhere in the text that was lexed,
the sequence of characters is the lexical error or last token lexed. the
$Tokens$ constructor is the case when legal tokens have been found. $currentSeq$
are all the currently lexed tokens save for the last, $lastToken$ are all the
possible ways that the last token can be lexed, in this implementation this is
refered to as the suffix and what it is and why it is needed will be explained
next.

\subsection{Suffix}
When a text is lexed it is uncertain that the last token is the actual end of
the file since it may be combined with something else. To ensure that all
possible outcomes will be handled the last token can be one of three different
forms.

\begin{itemize}
\item The part of the text lexed can end up in a legal state that is not
accepting.
\item The part of the text lexed can end up in an accepting state.
\item The part of the texted lexed can end in a legal state that is not
accepting, but the text can also be a sequence of multiple tokens.
\end{itemize}
To keep track of these cases a data structure that captures this was
implemented:
\lstinputlisting[language=Haskell]{examples/Suffix.hs}
The $Str$ contructor is used to keep track of partially complete tokens, an
example of this is when a string is started but the end quotation character have
not yet been found.

The $One$ constructor is used one exactly one token have been found, it may or
may not be the token that is used in the final result of the lexing. This
constructor can be omitted since the $Multi$ constructor can do the same job,
however it makes certain cases easier since the lexer can make assumptions that
can not be made for the $Multi$ constructor.

The $Multi$ constructor is used when atleast one token have been found but the
lexeme that is lexed in to the suffix can not be lexed to exactly one token. The
entire suffix still need to have a legal out state. This type of suffix can
typically be found when the begining of comments are lexed. for example the text
\emph{/*hello world} would be lexed to a sequence of complete tokens, but the
lexer still needs to keep track of the fact that it may still be a multi-line
comment. Note that in this case the $Tokens$ data structure would have one out
state and the suffix would have another.

\section{Transition Map}
The transition map is a function from an in state to $Tokens$. As shown above
the $Tokens$ data type contains the out state.
\lstinputlisting[language=Haskell]{examples/Transition.hs}
This data type is used in the lexical routines since the operations looks like
functional composition. The reason for using transition maps is that the lexer
doesn't know what the in state for a lexed text is, hence the tokens for all
possible in states must be stored. The transition map can be implemented in two
ways, an array format and a function composition format.

The array format uses an array to store the currently lexed tokens where the
index of the array represents the in state for that sequence of tokens. This is
useful when the tokens needs to be stored since it ensures that the tokens are
computed.

When combining lexed tokens it is useful to use functional composition since it
ensures that no unnecessary states will be computed. The drawback is that it
doesn't guarantee that the actual tokens are computed which may result in slow
performance at a later stage in the lexing.

Both these representations are used in the incremental divide and conquer lexer.
The array format is used when storing the tokens in the fingertree to allow for
fast access. The function composition is used when combining tokens to ensure
that only needed data is computed.

\section{Fingertree}
The fingertree is built up with the characters of the text being lexed as leafs
and the array format transition map as measure.
\lstinputlisting[language=Haskell]{examples/Fingertree.hs}
In order for the table data type to be a legal measure of the fingertree it has
to be a monoid.
\subsection{The Table Monoid}
The monoid class in Haskell have two different functions, $mempty$ which is the
identity element and $mappend$ which is an associative operator that describes how
two elements can be combined.
\lstinputlisting[language=Haskell]{examples/TableMonoid.hs}
There are two helper functions that convert between the array format that is
stored as the measure and the function composition format that is used in the
lexical routines.

As can be seen in the code, $mempty$ creates an array filled of empty $Tokens$.
In the $mappend$ case, every transition from the two arrays are extracted,
combined and put into a new array.

\section{Lexical routines}
%\subsection{combineTokens}
The lexical routines are divided into five functions. They each handles
different parts of the lexical steps that's needed in an incremental divide and
conquer lexer.
\lstinputlisting[language=Haskell]{examples/LexicalFunHeads.hs}

\textbf{combineTokens} is the function called when two fingertrees are combined.
\lstinputlisting[language=Haskell]{examples/CombineTokens.hs}
The function starts by checking if the tokens generated from $in\_state$ from
the first transition is empty or invalid in which case the output is trivial. If
it is not the tokens are passed on to $combineWithRHS$ together with the second
transition.

%\subsection{combineWithRHS}
\textbf{combineWithRHS} checks how the tokens from the first
transition is to be combined with the second transition.
\lstinputlisting[language=Haskell]{examples/CombineWithRHS.hs}
$combineWithRHS$ starts by creating tokens from the second transition using the
out state from the first tokens, this can result in three different cases:
\begin{description}
\item[isEmpty]If the tokens are empty the first tokens are returned.
\item[isValid]If the tokens are valid it means that the last token from the
first tokens can be combined with the first token in the second tokens into one
token.
\item[otherwise]If the tokens are not valid the lexer checks the suffix of the
first tokens to see if it ends in an accepting state, the $One$ construct, and
if so appends the tokens. If it is not invalid tokens will be returned.
\end{description}
The switch for $Multi$ extracts the tokens from the suffix and recursivly calls
\emph{combineWithRHS} to see if there is some other possible way to combine the
transitions.

%\subsection{mergeTokens}
\textbf{mergeTokens} combines the last token from the first tokens with
the first token of the second tokens.
\lstinputlisting[language=Haskell]{examples/MergeTokens.hs}
If there are more then one token in the second tokens the tokens are combined
into one token and the resto of the tokens in the second tokens are appended and
returned. If there is exactly one token in the second tokens two suffix is to be
combined. When two suffixes are combined some extra checks needs to be done.
If the second tokens has an accpeting out state, the two suffixes can be
combined into one token. If it is not the work is passed on to $mergeSuff$

%\subsection{mergeSuff}
\textbf{mergeSuff} checks which pair of suffix it has and takes the appropriate
actions.
\lstinputlisting[language=Haskell]{examples/MergeSuff1.hs}
If the first suffix is of type $Multi$ the function calls $combineWithRHS$. If
the resulting tokens are invalid a recursive call is made with the suffix from
the new tokens as first suffix.
\lstinputlisting[language=Haskell]{examples/MergeSuff2.hs}
If the first suffix is of type $Str$ the result will always be another $Str$ no
matter what is in the second suffix so the string is extracted and appended.
\lstinputlisting[language=Haskell]{examples/MergeSuff3.hs}
if the first suffix is of type $One$ and the second $Str$ a new $Multi$ suffix
is created. A new second tokens is created using the start state on the second
suffix, if this results in a valid $Tokens$ the token from the first suffix is
prepended. If it is not valid the $Str$ is just added to the end of the new
suffix.
\lstinputlisting[language=Haskell]{examples/MergeSuff4.hs}
When both suffix are $One$ they can be combined into a single token.
\lstinputlisting[language=Haskell]{examples/MergeSuff5.hs}
When the first suffix is $One$ and the second is $Multi$ it is passed onto
$mergeTokens$.

%\subsection{appendTokens}
\textbf{appendTokens} checks if there is a lexical error in the second tokens,
if it is that error is returned otherwise the second tokens is appended to the
first.
\lstinputlisting[language=Haskell]{examples/AppendTokens.hs}
