\chapter{Implementation}
In this chapter the tools, data structure and implementation of the incremental
divide and conquer lexer is explained. The implementation can be found in
\cref{chap:code}.

\section{Alex}
Alex is a Haskell tool for generating lexical analyzers given a
description of the language in the form of regular expressions, it is similar to
lex and flex in C and C++. The resulting lexer is Haskell 98 compatible and can
easily be used with the parser Happy, a parser generator for Haskell \cite{alex}.
Alex is notably used in BNFC which is a program to generate among other things a
lexer, parser and abstract syntax from Backus-Naur Form \cite{bnfc}.

\subsection{The DFA design}
The DFA used in the incremental lexer was created using Alex. The reason for
this is that Alex generates a DFA which is optimized in data size. Instead of
having an array for every possible character and state combination 5 arrays are
generated that takes advantage of the fact that for most characters the same
state will be used the majority of time. This saves a lot of elements that
would otherwise be the same in the array.

The trade of for using the Alex generated DFA is that some minor arithmetic
operations are used and some extra lookups are needed. These operations
are far less time consuming then the rest of the lexical operations.

\section{Token data structure}
To keep all the information that might be needed when combining two texts, a data
structure for the tokens was created. This data type contains more information
about the last token then what a sequential lexer would save, exactly what is
explained in \cref{sub:suff}.

%Rewrite this paragraph.
Since this project is about creating a real-time lexing tool, performance is
important. Therefor there are advantages of using sequences instead of lists.
The most notable place where this is used is in the measure of the fingertree,
where the tokens are stored in a sequence rather then a list. Sequences are also
used elsewhere in the project but this is the most notable place since it is
frequently updated.

\subsection{Tokens}
The internal structure used to store lexed tokens is called $Tokens$. There are
three constructors in the $Tokens$ data type.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/Token.hs}
  \caption{Tokens Data Type\label{fig:tokens}}
\end{figure}

$NoTokens$ is a representation of when an empty string has been lexed.
$InvalidTokens$ represents a lexical error somewhere in the text that was lexed,
the sequence of characters is the lexical error or last token lexed. the
$Tokens$ constructor is the case when legal tokens have been found. $currentSeq$
are all the currently lexed tokens save for the last, $lastToken$ are all the
possible ways that the last token can be lexed, in this implementation this is
referred to as the suffix and what it is and why it is needed will be explained
next.

\subsection{Suffix}\label{sub:suff}
When a text is lexed it is uncertain that the last token is the actual end of
the file since it may be combined with something else. To ensure that all
possible outcomes will be handled the last token can be on one of three different
forms.

\begin{itemize}
\item The part of the text lexed can end up in a legal state that is not
accepting.
\item The part of the text lexed can end up in an accepting state.
\item The part of the text lexed can end in a legal state that is not
accepting, but the text can also be a sequence of multiple tokens.
\end{itemize}
To keep track of these cases a data structure that captures this was
implemented, see \cref{fig:suff}.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/Suffix.hs}
  \caption{Suffix Data Type\label{fig:suff}}
\end{figure}

The $Str$ constructor is used to keep track of partially complete tokens, an
example of this is when a string is started but the end quotation character have
not yet been found.

The $One$ constructor is used one exactly one token have been found, it may or
may not be the token that is used in the final result of the lexing. Since this
constructor is a special case of the $Multi$ constructor it can be omitted.
However the $One$ constructor makes certain cases redundant since the lexer
makes assumptions that can not be made for the $Multi$ constructor.

The $Multi$ constructor is used when at least one token have been found but the
lexeme for the suffix does not match exactly one token. The entire suffix still
need to have a legal out state. This type of suffix can typically be found when
the beginning of a comment are lexed. for example the text
\emph{/*hello world} would be lexed to a sequence of complete tokens, but the
lexer still needs to keep track of the fact that it may be a multi-line
comment. Note that in this case the $Tokens$ data structure would have one out
state and the suffix would have another.

\section{Transition Map}
The transition map is a function from an in state to $Tokens$. As shown in
\cref{fig:tokens} the $Tokens$ data type contains the out state.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/Transition.hs}
  \caption{Transition Data Type \label{fig:transition}}
\end{figure}

This data type is used in the lexical routines. The reason for using transition
maps is that the lexer doesn't know what the in state for a lexed text is, hence
the tokens for all possible in states must be stored. The transition map can be
implemented in two ways, a table format and a function composition format.

The table format uses an array to store the currently lexed tokens where the
index of the array represents the in state for that sequence of tokens. This is
useful when the tokens needs to be stored since it ensures that the tokens are
computed.

When combining lexed tokens it is useful to use functional composition since it
ensures that no unnecessary states will be computed. The drawback is that it
doesn't guarantee that the actual tokens are computed which may result in slow
performance at a later stage in the lexing.

Both these representations are used in the incremental divide and conquer lexer.
The table format is used when storing the tokens in the fingertree to allow for
fast access. The function composition is used when combining tokens to ensure
that only needed data is computed.

\section{Fingertree}
The fingertree is built up with the characters of the text being the leafs
and the table format transition map as measure. In order for the table data type
to be a legal measure of the fingertree it has to be a monoid.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/Fingertree.hs}
  \caption{The data type for storing the tokens and text \label{fig:fingertreedt}}
\end{figure}

The monoid class in Haskell have two different functions, $mempty$ which is the
identity element and $mappend$ which is an associative operator that describes
how two elements are combined. As can be seen in \cref{fig:tablemonoid},
$mempty$ creates an array filled of empty $Tokens$. $mappend$ extracts the
functions from the old tables, combines them using $combineTokens$ then creates
a new table filled with the combination.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/TableMonoid.hs}
  \caption{The tabulate functions and monoid implementation \label{fig:tablemonoid}}
\end{figure}

There are two helper functions that convert between the table format that is
stored as the measure and the function composition format that is used in the
lexical routines.

\section{Lexical routines}
%\subsection{combineTokens}
The lexical routines are divided into five functions. They each handles
different parts of the lexical steps that is needed in an incremental divide and
conquer lexer.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/LexicalFunHeads.hs}
  \caption{Function definitions of the lexical routines \label{fig:funheads}}
\end{figure}

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/CombineTokens.hs}
  \caption{The $combineTokens$ function \label{fig:combinetoks}}
\end{figure}

\textbf{combineTokens} is the function called when two fingertrees are combined.
The function starts by checking if the tokens generated from $in\_state$ from
the first transition is empty or invalid in which case the output is trivial. If
it is not the tokens are passed on to $combineWithRHS$ together with the second
transition.

%\subsection{combineWithRHS}
\textbf{combineWithRHS} checks how the tokens from the first
transition is to be combined with the second transition.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/CombineWithRHS.hs}
  \caption{$CombineWithRHS$ function\label{fig:cwrhs}}
\end{figure}

$combineWithRHS$ starts by creating tokens from the second transition using the
out state from the first tokens, this can result in three different cases:
\begin{description}
\item[isEmpty]If the tokens are empty the first tokens are returned.
\item[isValid]If the tokens are valid it means that the last token from the
first tokens can be combined with the first token in the second tokens into one
token.
\item[otherwise]If the tokens are not valid the lexer checks the suffix of the
first tokens to see if it ends in an accepting state, the $One$ construct, and
if so appends the tokens. If it is not invalid tokens will be returned.
\end{description}
The switch for $Multi$ extracts the tokens from the suffix and recursively calls
\emph{combineWithRHS} to see if there is some other possible way to combine the
transitions.

%\subsection{mergeTokens}
\textbf{mergeTokens} combines the last token from the first tokens with
the first token of the second tokens.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/MergeTokens.hs}
  \caption{$MergeTokens$ function \label{fig:mergetokens}}
\end{figure}
If there are more then one token in the second tokens the tokens are combined
into one token and the rest of the tokens in the second tokens are appended and
returned. If there is exactly one token in the second tokens two suffix is to be
combined. When two suffixes are combined some extra checks needs to be done.
If the second tokens has an accepting out state, the two suffixes can be
combined into one token. If it is not the work is passed on to $mergeSuff$

%\subsection{mergeSuff}
\textbf{mergeSuff} checks which pair of suffix it has and takes the appropriate
actions.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/MergeSuff1.hs}
  \caption{$MergeSuff$ for $Multi$ \label{fig:msmulti}}
\end{figure}

If the first suffix is of type $Multi$ the function calls $combineWithRHS$. If
the resulting tokens are invalid a recursive call is made with the suffix from
the new tokens as first suffix.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/MergeSuff2.hs}
  \caption{$MergeSuff$ for $Str$ \label{fig:msstr}}
\end{figure}

If the first suffix is of type $Str$ the result will always be another $Str$ no
matter what is in the second suffix so the string is extracted and appended.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/MergeSuff3.hs}
  \caption{$MergeSuff$ for $One$ and $Str$ \label{fig:msonestr}}
\end{figure}

if the first suffix is of type $One$ and the second $Str$ a new $Multi$ suffix
is created. A new second tokens is created using the start state on the second
suffix, if this results in a valid $Tokens$ the token from the first suffix is
prepended. If it is not valid the $Str$ is just added to the end of the new
suffix.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/MergeSuff4.hs}
  \caption{$MergeSuff$ for $One$ and $One$ \label{fig:msoneone}}
\end{figure}

When both suffix are $One$ they can be combined into a single token.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/MergeSuff5.hs}
  \caption{$MergeSuff$ for $One$ and $Multi$ \label{fig:msonemulti}}
\end{figure}

When the first suffix is $One$ and the second is $Multi$ it is passed onto
$mergeTokens$.

%\subsection{appendTokens}
\textbf{appendTokens} checks if there is a lexical error in the second tokens,
if it is that error is returned otherwise the second tokens is appended to the
first.
\lstinputlisting[language=Haskell]{examples/AppendTokens.hs}
