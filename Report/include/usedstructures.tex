\chapter{Implementation}\label{chap:imp}
In this chapter the tools, data structure and implementation of the incremental
divide and conquer lexer is explained. The implementation of the incremental
divide and conquer lexer uses fingertrees for storing the intermediate tokens
and the lexed text. It has an internal representation of the tokens to keep
track of the data needed when two fingertrees are combined. The lexical routines
for combining the internal token data type take advantage of functional
composition in order to get lazy updating of the tokens when two fingertrees are
combined. The complete implementation can be found in \cref{chap:code}.

%\section{Alex}
\section{The DFA Design}
The DFA used in the incremental lexer was created using Alex. Alex is a Haskell
tool for generating lexical analyzers given a
description of the language in the form of regular expressions, it is similar to
lex and flex in C and C++. The resulting lexer is Haskell 98 compatible and can
easily be used with the parser Happy, a parser generator for Haskell \cite{alex}.
Alex is notably used in BNFC which is a program to generate among other things a
lexer, parser and abstract syntax from Backus-Naur Form \cite{bnfc}.

The reason for using Alex to generate the DFA is that it optimizes the number of
elements in the transition table. Instead of
having an array for every possible character and state combination, 5 arrays are
generated that takes advantage of the fact that for most characters the same
state will be used the majority of time. This saves a lot of elements that
would otherwise be the same in the array.

The trade of for using the Alex generated DFA is that some minor arithmetic
operations are used and some extra lookups are needed. These operations
are far less time consuming then the rest of the lexical operations.

\section{Token data structure}
To keep all the information that might be needed when combining two texts, a data
structure for the tokens was created. This data type contains more information
about the last token then what a sequential lexer would save, exactly what is
explained in \cref{sub:suff}.

%Rewrite this paragraph.
Since this project is about creating a real-time lexing tool, performance is
important. Therefor there are advantages of using sequences instead of lists.
The most notable place where this is used is in the measure of the fingertree,
where the tokens are stored in a sequence rather then a list. Sequences are also
used elsewhere in the project but the measure is the most notable place since it is
frequently updated.

\subsection{Tokens}
The internal structure used to store lexed tokens is called $Tokens$. There are
three constructors in the $Tokens$ data type, see \cref{fig:tokens}.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/Token.hs}
  \caption{Tokens Data Type\label{fig:tokens}}
\end{figure}

$NoTokens$ is a representation of when an empty string has been lexed.
$InvalidTokens$ represents a lexical error somewhere in the text that was lexed,
the sequence of characters is the lexical error or last token lexed. The
$Tokens$ constructor is the case when legal tokens have been found. $currentSeq$
are all the currently lexed tokens save for the last, $lastToken$ are all the
possible ways that the last token can be lexed, in this implementation this is
referred to as the suffix and what it is and why it is needed will be explained
next.

\subsection{Suffix}\label{sub:suff}
When a text is lexed it is uncertain that the last token is the actual end of
the file since it may be combined with something else. To ensure that all
possible outcomes will be handled the last token can be on one of three
different forms. The part of the text lexed can end in:
\begin{itemize}
\item a legal state that is not accepting.
\item an accepting state.
\item a legal state that is not accepting, but the text can also be a sequence
  of multiple tokens.
\end{itemize}
To keep track of these cases a data structure that captures this was
implemented, see \cref{fig:suff}.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/Suffix.hs}
  \caption{Suffix Data Type\label{fig:suff}}
\end{figure}

The $Str$ constructor is used to keep track of partially complete tokens, an
example of this is when a string is started but the end quotation character have
not yet been found.

The $One$ constructor is used one exactly one token have been found, it may or
may not be the token that is used in the final result of the lexing. Since this
constructor is a special case of the $Multi$ constructor it can be omitted.
However the $One$ constructor makes certain cases redundant since the lexer
makes assumptions that can not be made for the $Multi$ constructor.

The $Multi$ constructor is used when at least one token have been found but the
lexeme for the suffix does not match exactly one token. The entire suffix still
need to have a legal out state. This type of suffix can typically be found when
the beginning of a comment are lexed. for example the text \emph{/*hello world}
would be lexed to a sequence of complete tokens, ``/'', ``*'', ``hello'' and
``world'', but the lexer still needs to keep track of the fact that it may be in
the middle of a multi-line comment. Note that in this case the $Tokens$ data
structure would have one out state, the state for the middle of a comment, and
the suffix would have another, the end of an ident.

\section{Transition Map}
The transition map is a function from an in state to $Tokens$. As shown in
\cref{fig:tokens} the $Tokens$ data type contains the out state.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/Transition.hs}
  \caption{Transition Data Type \label{fig:transition}}
\end{figure}

This data type is used in the lexical routines. The reason for using transition
maps is that the lexer does not know what the in state for a lexed text is, hence
the tokens for all possible in states must be stored. The transition map can be
implemented in two ways, a table format and a function composition format.

The table format uses an array to store the currently lexed tokens where the
index of the array represents the in state for that sequence of tokens. This is
useful when the tokens needs to be stored since it ensures that the tokens are
computed.

When combining lexed tokens it is useful to use functional composition since it
ensures that no unnecessary states will be computed. The drawback is that it
does not guarantee that the actual tokens are computed which may result in slow
performance at a later stage in the lexing.

Both these representations are used in the incremental divide and conquer lexer.
The table format is used when storing the tokens in the fingertree to allow for
fast access. The function composition is used when combining tokens to ensure
that only needed data is computed.

\section{Fingertree}
The fingertree is constructed with the characters of a text being the leaves and
with the table format transition map as it is measure. The $Table$ data type has
to be a monoid in order to be a legal measure of the fingertree.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/Fingertree.hs}
  \caption{The data type for storing the tokens and text \label{fig:fingertreedt}}
\end{figure}

The monoid class in Haskell have two different functions, $mempty$ which is the
identity element and $mappend$ which is an associative operator that describes
how two elements are combined. As can be seen in \cref{fig:tablemonoid},
$mempty$ creates an array filled of empty $Tokens$. $mappend$ extracts the
functions from the old tables, combines them using $combineTokens$ then creates
a new table filled with the combination.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/TableMonoid.hs}
  \caption{The tabulate functions and monoid implementation \label{fig:tablemonoid}}
\end{figure}

There are two helper functions that convert between the table format that is
stored as the measure and the function composition format that is used in the
lexical routines.

\section{Lexical routines}
%\subsection{combineTokens}
The lexical routines are divided into five functions. They each handle
different parts of the lexical steps that are needed in an incremental divide and
conquer lexer.

%\begin{figure}[h!]
%  \lstinputlisting[language=Haskell]{examples/LexicalFunHeads.hs}
%  \caption{Function definitions of the lexical routines \label{fig:funheads}}
%\end{figure}

\subsection{Combination of Tokens}
\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/CombineTokens.hs}
  \caption{The $combineTokens$ function \label{fig:combinetoks}}
\end{figure}

$combineTokens$ is the function called when two fingertrees are combined.
The function starts by checking if the tokens generated from $in\_state$ from
the first transition is empty or invalid in which case the output is trivial. If
the tokens generated are valid, the tokens are passed on to $combineWithRHS$
together with the second transition.

\subsection{Combine Tokens With Right Hand Side}
$combineWithRHS$ checks how the tokens from the first
transition are to be combined with the second transition.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/CombineWithRHS.hs}
  \caption{$CombineWithRHS$ function\label{fig:cwrhs}}
\end{figure}

$combineWithRHS$ starts by creating tokens from the second transition, $toks2$,
using the out state from the first tokens, this can result in three different
cases, the definition of the variable names can be found in \cref{fig:cwrhs}.
\begin{description}
\item[isEmpty]If $toks2$ is empty $toks1$ is returned.
\item[isValid]If $toks2$ is valid it means that the last token from the
  $toks1$ can be combined into one token with the first token in $toks2$.
\item[otherwise]If $toks2$ is not valid the lexer checks the suffix of $toks1$
  to see if it ends in an accepting state or a valid state.
  \begin{itemize}
  \item if the $One$ constructor is found the suffix ends in an accepting state
    which means that tokens created from the start state can be appended to
    $toks1$.
  \item If the $Multi$ constructor is found the tokens from the suffix,
    $suffToks$, is extracted and a recursive call to $combineWithRHS$ is made
    with $suffToks$ as argument instead.
  \item If the $Str$ constructor is found the suffix does not end in a valid
    state and $InvalidTokens$ will be returned.
  \end{itemize}
\end{description}
%The switch case for $Multi$ extracts the tokens from the suffix and recursively
%calls \emph{combineWithRHS} to see if there is some other possible way to
%combine the transitions.

\subsection{Merge Two Tokens}
$mergeTokens$ combines the last token from the first tokens with
the first token of the second tokens, for the code see \cref{fig:mergetokens}.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/MergeTokens.hs}
  \caption{$MergeTokens$ function \label{fig:mergetokens}}
\end{figure}
\begin{itemize}
\item If there are more then one token in $toks2$, $suff1$ is combined into one
  token with the first token in $toks2$ and the rest of the tokens in $toks2$
  is appended and returned.
\item If there is exactly one token in $toks2$, the suffix from $toks1$ is
  combined with $suff1$. When two suffixes are combined some extra checks are
  needed. If $toks2$ has an accepting out state, the two suffixes
  can be combined into one token. If $toks2$ does not have an accepting out
  state the work is passed on to $mergeSuff$.
\end{itemize}

\subsection{Merging Suffixes}
$mergeSuff$ checks which pairs of suffixes it has and takes the appropriate
actions.

\begin{figure}[h!]
  \lstinputlisting[language=Haskell]{examples/MergeSuff.hs}
  \caption{$MergeSuff$ function \label{fig:msuff}}
\end{figure}
\begin{itemize}
\item If the first suffix is of type $Multi$ the function calls
  $combineWithRHS$. If the resulting tokens is invalid a recursive call is made
  with the suffix from the new tokens as first suffix.
\item If the first suffix is of type $Str$ the result will always be another
  $Str$ no matter what is in the second suffix so the string is extracted and
  appended.
\item if the first suffix is of type $One$ and the second $Str$ a new $Multi$
  suffix is created. A new second tokens is created using the start state on the
  second suffix, if this results in a valid $Tokens$, the token from the first
  suffix is prepended. If it is not valid the $Str$ is just added to the end of
  the new suffix.
\item When both suffix are $One$ they can be combined into a single token.
\item When the first suffix is $One$ and the second is $Multi$ it is passed onto
  $mergeTokens$.
\end{itemize}
\subsection{Append to Sequence of Tokens}
$appendTokens$ checks if there is a lexical error in $toks2$. if there is
an error, that error is returned, otherwise $toks2$ is appended to $toks1$.
\lstinputlisting[language=Haskell]{examples/AppendTokens.hs}
