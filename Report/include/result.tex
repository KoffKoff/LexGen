\chapter{Result}
The incremental lexer has as mentioned before three requirements, it should be
Robust, Efficient and Precise. Robustness means that the lexer does not crash
when it encounter an error in the syntax. That is, if a string would yield an
error when lexed from the starting state the lexer does not return that error but
instead stores the error and lexes the rest of the possible input states since
the current string might not be at the start of the code.

For it to be efficient
the feedback to the user must be instant, or more formally the combination of two
strings should be handled in $O(log(n))$ time.

Finally to be precise the lexer
must give a correct result. This chapter will talk about how these requirements
are tested and what the results were.

In the sections Below, any mention of a sequential lexer refers to a lexer
generated by Alex using the same alex file as was used when creating the
incremental lexer \cite{alex}. The reason why Alex was used is because the DFA
generated by Alex was used in the incremental lexer, thus ensuring that only the
lexical routines differs.
%\section{Robustness}

\section{Preciseness}
For an incremental lexer to work, the lexer must be able to do lexical analysis
of any sub text of a text and be able to combine two sub texts. If the lexical
analysis of one sub text does not result in any legal tokens it must be able to be
combined with other sub texts that makes it legal tokens. The lexical analysis of
a sub text might not always result in the same tokens that the combination of the
sub text with another text would give.

To test these cases a test was constructed that does a lexical analysis on two
sub texts using the incremental lexer and then combining the results into one
text. The result of the combination should be the same as the lexical analysis
of the text using the incremental lexer and the result using a sequential lexer.

It is not enough to test if the combination of two sub texts yields the
same sequence of tokens as the text. To test that the result of the
incremental lexer is the correct sequence of tokens generated, it is compared to
what a sequential lexer generates. This comparison is an equality test of
the text, it checks token for token that they are the same kind of token and
have the same lexeme.
\cref{fig:CheckEquility} shows the test for equality:
\begin{figure}[h!]
  \centering
  \lstinputlisting[language=c]{examples/CorrectEquility.hs}
  notEqual function is a function which pattern-match on the two different
  tokens and returns true if they are not of the same type.
  \caption{Code for testing tokens from IncLex is equal to tokens from Alex. 
  \label{fig:CheckEquility}}
\end{figure} 

%\section{Efficiency}
\section{Performance}
To measure the performance of the incremental step we created the fingertree for
two pieces of code. By creating the two fingertrees the transition map for the
code in those trees are created as well. The benchmarking was then done on the
combination of the two trees. The results of the incremental lexer benchmarking
suggests a running time of $O(log(n))$.

To get a reference point the same text was lexed using a sequential lexer.

Since the incremental lexer uses a tree to store the data, where the plain chars
of the text is kept in the lowest level. An increased usage of data-space than
what is used in a sequential lexer is expected. The expected space complexity is
$\Theta(mn \log n)$ since the every level of the tree contains $n$ elements and $m$ states and
there are $\log n$ levels in the tree.

\#Each level has token data structure for n characters, 1 character roughly 100 Byte for a DFA with 90 states according in token data structure according to our estimates.

\#Lägga till nåt om minnes utrymmet som krävs.

\#Bättre förklaring till graferna.

\#Tester för nybyggning av träd?

\begin{filecontents*}{data.csv}
%characters*1000, time in microsecs
n    alex   upd
1    210    150
10   1901   251
20   3910   329
30   5303   361
40   7079   361
50   8933   367
60   12140  460
70   14860  469
80   16850  523
90   17170  424
100  20920  554
110  24960  486
120  26430  503
130  28420  649
140  37890  575
150  33790  570
160  36850  624
170  39860  468
180  42570  496
190  42410  513
\end{filecontents*}
\begin{example}[Benchmarking times of the incremental and sequential lexer]\label{bench}
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
    width=0.7\textwidth,
    height=0.4\textwidth,
    ymajorgrids,
    title = {Comparison},
    scaled y ticks=real:1e3,
    x tick label style={/pgf/number format/1000 sep=},
    ytick scale label code/.code={$ms$},
    legend pos=outer north east
    ]
        \addplot table[x=n,y=upd] {data.csv};\addlegendentry{Incremental update}
        \addplot table[x=n,y=alex] {data.csv};\addlegendentry{Sequential}
    \end{axis}
\end{tikzpicture}
\end{center}
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
    width=0.7\textwidth,
    height=0.4\textwidth,
    ymajorgrids,
    title = {Incremental lexer},
    scaled y ticks=real:1,
    x tick label style={/pgf/number format/1000 sep=},
    ytick scale label code/.code={$\mu s$},
    legend pos=outer north east
    ]
        \addplot table[x=n,y=upd] {data.csv};\addlegendentry{Incremental update}
    \end{axis}
\end{tikzpicture}
\end{center}
\end{example}
