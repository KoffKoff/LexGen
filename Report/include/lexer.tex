\chapter{Lexer \label{chap:lexer}}
A lexer, lexical analyzer, is a pattern matcher. Its job is to divide a text
into a sequence of tokens (such as words, punctuation and symbols). A Lexer is
often used as the first stage of a syntax analyzer \cite{sebesta2012}. The syntax
analyzer in turn takes the tokens generated by the lexer and returns a set of
expressions and statements. Lexing can be done by using regular expressions,
regular sets and finite automata, which are all fundamental notions in
formal language theory \cite{Aho1990}. The rest of this chapter describes the
concepts of the lexer in detail.

\section{Lexing vs Parsing}
Lexers usually work as a pass before parser; giving their result to the syntax
analyzer. Splitting up a parser into a number of different tasks have several
benefits. Here follows some from breaking out the lexical analyzer from the
syntactical analyzer.

Firstly, a clean design. If striping the lexical analyse part out of a parser,
the parsing step can be designed in a cleaner way. Also the lexical analyse can
be designed in a cleaner and smarter way. A Lexer can ignore to pass along
unneeded (for the parser) data, like white spaces and comments. This opens up
for a cleaner design of a programming language. The only thing the syntax
analyzer will see is the output from the lexer, tokens (and lexemes). The lexer
usually skips comments and white-spaces, since these are often not relevant for
the syntax analyzer \cite{Aho2006}.

Secondly, splitting a big problem into smaller specific sub-problems opens up
for an efficient problem solving. Specific problems can use specialized techniques
,which are often optimized, to solve the problem. This means that the efficiency
of the parser can be improved \cite{Aho2006}.

Thirdly and last, breaking out the lexical part makes it possible for the
syntactical part to solve its problem in a generic way. When changing language
to parse, the only part that needs to be changed in the parser is the lexical
part. This opens up for portability. Illegal character sequences can be stopped
in the lexical part of the parser \cite{Aho2006}. Therefore the lexer can detect
syntactical errors in tokens, such as ill-formed floating-points literals, and
report these errors to the user \cite{sebesta2012}. Finding these errors allows
the compiler to break the compilation before running the syntax analyzer,
thereby saving computing time.

\section{Token Specification}
The job of the lexical analyzer is to translate a human readable text to an
abstract computer-readable list of tokens. There are different techniques a
lexer can use when finding the abstract tokens representing a text. This section
describes the techniques used when writing rules for the tokens patterns.

\subsection{Regular Expressions}
Regular expressions is a key part in describing patterns of a text. However,
they cannot express all possible patterns on which a text can follow, but they
can describing those type of pattern which are used in a lexer, the pattern of
tokens.

\begin{definition}[Regular Expressions \cite{Aho1990}]\label{regexp} $ $\\
\begin{enumerate}
  \item The following characters are meta characters: $meta = \{ '|', ~~ '(', ~~ ')', ~~ '*' \}$.
  \item A character $a \notin meta$ is a regular expression that matches the 
      string $a$.
  \item If $r_1$ and $r_2$ are regular expressions then $(r_1 | r_2)$ is a 
      regular expression that matches any string that matches $r_1$ or $r_2$.
  \item If $r_1$ and $r_2$ are regular expressions. $(r_1)(r_2)$ is a regular
      expression that matches the string $xy$ iff $x$ matches $r_1$
      and $y$ matches $r_2$.
  \item If $r$ is a regular expression $r*$ is a regular expression that
      matches any string of the form $(x_1)(x_2) \dots  (x_n), n \geq 0$;
      where $X_i$ matches $r$ for $1 \leq i \leq n$, in particular $(r)*$ 
      matches the empty string, $\varepsilon$.
  \item If $r$ is a regular expression, then $(r)$ is a regular expression that
      matches the same string as $r$.
\end{enumerate}
\qeda
\end{definition}

By introducing a priority level and associativity to the different operators,
parentheses can be eliminated. The operator with the highest priority is the $*$
operator. The second highest is the \emph{concat} operator $(r_1)(r_2)$ and the
operator with the lowest level is the \emph{or} operator $|$. The $*$ operator
can not have a associativity since it only takes one argument. The other two
\emph{binary} operators \emph{concat} and \emph{or} are left-associative
\cite{Aho1990}.

Many parentheses can be omitted by adopting the convention that the \emph{Kleene
closure} operator $*$ has the highest precedence, the \emph{concat} operator
$(r_1)(r_2)$ the second highest and last the \emph{or} operator $|$. The two
binary operators, \emph{concat} and \emph{or} are left-associative \cite{Aho1990}.

\begin{example}[Valid C Idents \cite{Aho2006}]\label{regexpEx}
Using regular expressions to express a set of valid C identifiers is easy.
Given an element $letter \in \{$a$ \dots $z$\} \cup \{$A$ \dots $Z$\} \cup 
\{$\_$\}$ and another element $digit \in \{0 \dots 9\}$ then using a regular
expression, the definition of all valid C identifiers 
could look like this: $letter (letter | digit)*$. 
\end{example}

\subsection{Languages}
An alphabet is a finite set of symbols. For example Unicode includes
approximately $100,000$ characters. A language is any countable set of strings
of some fixed alphabet \cite{Aho2006}. The term formal language refers to
languages which can be described by a body of systematic rules. There is a
subset of languages to formal languages called regular languages, these regular
languages refers to those languages that can be defined by regular expressions
\cite{Ranta2012}.

\subsection{Regular Definitions}
When defining a language it is useful to give the regular expressions names,
so they can for example be used in other regular expressions. These names for
the regular expressions are themselves symbols. If $\Sigma$ is an alphabet of
basic symbols, then a regular definition is a sequence of definitions of the
form:
\begin{center}
\begin{tabular}{l c r}
$d_1$ & $\to$ & $r_1$\\
$d_2$ & $\to$ & $r_2$\\
$\vdots$ & $\to$ & $\vdots$\\
$d_n$ & $\to$ & $r_n$\\

\end{tabular}
\end{center}
where:
\begin{enumerate}
\item Each $d_i$ is a new symbol, not in $\Sigma$ and not the same as any other
of the $d$'s.
\item Each $r_i$ is a regular expression over the alphabet $\Sigma  \cup \{d_1,
d_2 \dots d_{i-1}\}$
\end{enumerate}
By restricting $r_i$ to $\Sigma$ and previously defined $d$'s the regular
definitions avoid recursive definitions \cite{Aho2006}.

\section{Tokens, Patterns and Lexemes}
When rules have been defined for a language, the lexer needs structures to
represent the rules and the result from lexing the text.
This section describes the structures which the lexical analyzer uses
for representing the abstract data; what these structures are used for and what is
forwarded to the syntactical analyzer.
\newpage
A lexical analyzer uses three different concepts. The concepts are described
below.
\begin{itemize}
  \item A \textbf{token} is a pair consisting of a token name and an optional attribute value. The
token name is an abstract symbol corresponding to a lexical unit \cite{Aho2006}.
For example, a particuar keyword, data-type or identifier.
  \item A \textbf{pattern} is a description of what form a lexeme may take \cite{Aho2006}.
For example, a variable or identifier is commonly formed by a letter followed by
a sequence of letters, digits and \_, an integer is a sequence consisting of
digits from 0 to 9. This can be described by a regular expression.
  \item A \textbf{lexeme} is a sequence of characters in the code being analyzed which
matches the pattern for a token and is identified by the lexical analyzer as an
instance of a token \cite{Aho2006}.
\end{itemize}
As mentioned before, a token consists of a token name and an optional attribute value.
This attribute is used when one pattern can match more then one lexeme.
For example the pattern for a digit token matches both $0$ and $1$,
but it is important for the code generator to know which lexeme was found.
Therefore the lexer often returns not just the token but also an attribute value
that describes the lexeme found in the source program corresponding to this
token \cite{Aho2006}.

A lexer collects chars into logical groups and assigns 
internal codes to these groups according to their structure, 
where the groups of chars are lexemes and the internal codes are tokens \cite{sebesta2012}.
In some cases it is not relevant to return a token for a pattern, in these
cases the token and lexeme is discarded and the lexer continues,
typical examples are whitespaces and comments which have no impact on
the code \cite{Aho2006}.

\begin{figure}[h!]
\begin{center}
\begin{grammar}

<letter>  $\in$ \{`a' - `z'\} $\cup$ \{`A' - `Z'\} $\cup$ \{`_'\}

<digit>  $\in$ \{0 - 9\}

<identifier> ::= <letter> (<letter> | <digit>)* 

<integer> ::= <digit>+

<multi-line comment> ::= `/*' ([$\wedge$ `*'] | `*' [$\wedge$ `/'])* `*/'

<reserved-words> ::= `(' | `)' | `{' | `}' | `;' | `=' | `++' | `<' | `+' | `-' | `*'

\end{grammar}
\caption{Grammar rules for \cref{codeToToken} \& \cref{longestMatch}\label{fig:grammar}}
\end{center}
\end{figure}

An example follows how a small piece of code would be divided given the regular
language described in \cref{reglang}.

\newpage

\begin{example}[Logical grouping \cite{sebesta2012}] \label{codeToToken}$ $\\
Consider the following text; to be lexed:
\lstinputlisting[language=c]{examples/token.c}
Given the regular language defined in \cref{reglang}, the lexical analyzer would
use the rules defined in \cref{fig:grammar} and produce the resulting
tokens shown in \cref{fig:codeToToken}.

\begin{figure}[h!]
\begin{center}
\begin{tabular}{l c}
\underline{Token} & \underline{Lexeme}\\
Identifier & result\\
Reserved & $=$\\
Identifier & oldsum\\
Reserved & $-$\\
Identifier & value\\
Reserved & $/$\\
Integer & 100\\
Reserved & ;
\end{tabular}
\end{center}
\caption{Result of lexing the code in \cref{codeToToken} \label{fig:codeToToken}}
\end{figure}
\end{example}

\section{Recognition of Tokens}
The topic in the previous section covers how to represent a pattern using
regular expressions and how these expressions relate to tokens. A pattern is
used to determine if a string matches a token. This section is highlighting how
to transform a sequence of characters into a sequence of abstract tokens using
patterns.

\subsection{Transition Diagrams}
A transition diagram is a directed graph, where the
nodes are labeled with state names. Each node represents a state which could
occur during the process of scanning the input, looking for a lexeme that
matches one of several patterns \cite{Aho2006}. The edges are labeled with the
input characters that causes transitions among the states. An edge may also
contain actions that the lexer must perform when the transition is a token
\cite{sebesta2012}.

There are different types of states in the the transition diagram. One
state is said to be the initial state. The transition diagram always begins at
this state, before any input symbols have been read. Some states are said to be
accepting (final). They indicate that a lexeme has been found. If the token
found is the longest match (see \cref{sub:longmatch}) then the token will be
returned with any additional optional values, mentioned in previous section, and
the transition is reset to the initial state \cite{Aho2006}.

\subsection{Longest Match}\label{sub:longmatch}
If there are multiple feasible solutions when performing the lexical
analysis, the lexer will return the token that is the longest. To manage this
the lexer will continue in the transition diagram if there are any legal edges
leading out of the current state, even if it is an accepting state \cite{Aho2006}.

The above rule is not always enough since the lexer has to explore all legal
edges, even if the current state is accepting. If the lexer is in a state that
is not accepting and do not have any legal edge out of that state, the lexer
can not return a token. To solve this the lexer has to keep track of what the
latest accepting state was. When the lexer reaches a state with no
legal edge out of it, the lexer returns the token corresponding to the last
accepting state. The tail of the string, the part that was not in the returned
token, is then lexed from the initial state as part of a new token
\cite{Aho2006}.

\begin{example}[Longest Match] \label{longestMatch}
Consider the following text; to be lexed.
\lstinputlisting[language=c]{examples/longesttoken.c}
Allthough this piece of C code is not syntactically correct, there are no
lexical errors in it. Since
the text starts with a multi line comment sign the lexer will try to lex it as
a comment. When the lexer encounters the end of the text it will return the
token corresponding to the last accepting state and begin lexing the rest from
the initial state. The rules relevant to this example are defined in 
\cref{fig:grammar} the rest of the rules can be found in \cref{reglang}.\\
The result can be found in \cref{fig:longestmatch}.
\end{example}

\begin{figure}[h!]
\begin{center}
\begin{tabular}{l c}
\underline{Token} & \underline{Lexeme}\\
Reserved & $/$\\
Reserved & $*$\\
Identifier & result\\
Reserved & $=$\\
Identifier & oldsum\\
Reserved & $-$\\
Identifier & value\\
Reserved & $/$\\
Integer & 100\\
Reserved & ;
\end{tabular}
\end{center}
\caption{Result of lexing the code in \cref{longestMatch} \label{fig:longestmatch}}
\end{figure}


\subsection{Finite Automata}
Transition diagrams of the form used in lexers are representations of a class 
of mathematical machines called finite automata. Finite automata can be 
designed to recognize members of a class of languages called regular languages, 
mentioned above \cite{sebesta2012}.
A finite automaton is essentially a graph, like transitions diagrams, with some 
differences:
\begin{itemize}
  \item Finite automata are recognizers; they say "YES" or "NO" about 
each possible input string.
  \item Finite automata come in two different forms:
    \begin{description}
      \item [Non-deterministic Finite Automata (NFA)] which have no restriction 
of the edges, several edges can be labeled by the same symbol out from the 
same state. Further, the empty string $\epsilon$ is a possible label. 
      \item [Deterministic Finite Automata (DFA)] for each state and for each 
symbol of its input alphabet exactly one edge with that symbol leaving that 
state. The empty string $\epsilon$ is not a valid label.
    \end{description}
\end{itemize}
Both these forms of finite automata are capable of recognizing the same 
subset of languages, all regular languages \cite{Aho2006}.

\subsubsection{Non-deterministic Finite Automata}
An NFA accepts the input $x$ if and only if there is a path in the transition 
diagram from the start state to one of the accepting states, such that the 
symbols along the way spells out $x$ \cite{Aho2006}.
The formal definition of a non-deterministic finite automaton follows:
\begin{definition}[Non-deterministic Finite Automata \cite{sipser2006}] \label{finiteAutomataDef}
A finite automata is a 5-tuple $(Q, \Sigma, \delta, q_0, F)$, where
\begin{enumerate}
  \item $Q$ is a finite set called the states,
  \item $\Sigma$ is a finite set called alphabet,
  \item $\delta: Q \times \Sigma \to P(Q)$ is a transition function,
  \item $q_0 \in Q$ is the start state, and
  \item $F \subseteq Q$ is the accepting states.
\end{enumerate} 
\end{definition}
The transition function does not map to one particular state from a
state and element tuple. This is because one state may have more than one edge
per element. An example of this can be seen in \cref{regexp2td}.

There are two different ways of representing an NFA which this report will
describe. One is by transition diagrams, where the regular expression will be
represented by a graph structure. Another is by transitions table, where the 
regular expression will be converted into a table of states and the 
transitions for these states given the input. The following examples shows how 
the transition diagram and transition table representation will look like for a 
given regular expression.

\begin{example}[RegExp to Transition Diagram \& Transition Table \cite{Aho2006}] \label{regexp2td}
Given this regular expression:
\begin{center}
    $(a | b)* abb$ 
\end{center}

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[
    ->,>=stealth',shorten >=1pt,auto,
    node distance=2cm,
    semithick
    ]

    \newState{0}{$0$}{initial}{}
    \newState{1}{$1$}{right of=0}{}
    \newState{2}{$2$}{right of=1}{}
    \newState{3}{$3$}{right of=2}{accepting} 

    \newTransition{0}{0}{a}{loop above}
    \newTransition{0}{0}{b}{loop below}
    \newTransition{0}{1}{a}{}
    \newTransition{1}{2}{b}{}
    \newTransition{2}{3}{b}{}
  \end{tikzpicture}
  \caption{Transition Diagram, accepting the pattern $(a | b)* abb$
  \label{fig:td}}
\end{figure}

The transition diagram in \cref{fig:td} is representing this regular expression.

\begin{figure}[h!]
  \centering
  \begin{tabular}{| c | c c c |}
    \hline
    \hline
    State & a & b & $\epsilon$\\
    \hline
    0 & $\{0, 1\}$ & $\{0\}$ & $\emptyset$ \\
    1 & $\emptyset$ & $\{2\}$ & $\emptyset$ \\
    2 & $\emptyset$ & $\{3\}$ & $\emptyset$ \\
    3 & $\emptyset$ & $\emptyset$ & $\emptyset$ \\
    \hline
  \end{tabular}
  \caption{Transition Table, accepting the pattern $(a | b)* abb$
  \label{fig:tt}}
\end{figure}

It could also be converted into the transition table shown in \cref{fig:tt}
\end{example}

Transition tables have the advantage that they have a quick lookup time. But 
instead it will take a lot of data space, when the alphabet is large. Most 
states do not have any moves on most of the input symbols \cite{Aho2006}.
\subsubsection{Deterministic Finite Automata}
DFA is a special case of an NFA where,
\begin{enumerate}
  \item there are no moves on input $\epsilon$ and
  \item for each state $s$ and input symbol $a$, there is exactly one edge out
        of $s$ labeled with $a$.
\end{enumerate}
A NFA is one abstract representation of an algorithm to recognize a string 
in one language, the DFA is a simple concrete algorithm for recognizing strings. 
Every regular expression can be converted into a NFA. Every NFA can be 
converted into a DFA and then converted back to a regular expression \cite{Aho2006}. 
It is the DFA that is implemented and used when building lexical analyzers.
The formal definition of a deterministic finite automaton follows:
\begin{definition}[Deterministic Finite Automata \cite{sipser2006}] \label{finiteAutomataDef}
A finite automata is a 5-tuple $(Q, \Sigma, \delta, q_0, F)$, where
\begin{enumerate}
  \item $Q$ is a finite set called the states,
  \item $\Sigma$ is a finite set called alphabet,
  \item $\delta: Q \times \Sigma \to Q$ is a transition function,
  \item $q_0 \in Q$ is the start state, and
  \item $F \subseteq Q$ is the set of accepting states.
\end{enumerate} 
\end{definition}

\begin{example}[DFA representation of RegExp \cite{Aho2006}] \label{regexp2dfa}
A DFA representation of the regular expression from \cref{regexp2td} is shown in \cref{fig:dfa}
\end{example}
\begin{figure}[!h]
  \centering
  \begin{tikzpicture}[
    % Default arrow tip
    ->,>=stealth',shorten >=1pt,auto,
    % Default node distance
    node distance=2cm,
    % Edge stroke thickness: semithick, thick, thin
    semithick
    ]

    \newState{0}{$0$}{initial}{}
    \newState{1}{$1$}{right of=0}{}
    \newState{2}{$2$}{right of=1}{}
    \newState{3}{$3$}{right of=2}{accepting} 

    \newTransition{0}{0}{b}{loop above}
    \newTransition{0}{1}{a}{}
    \newTransition{1}{1}{a}{loop below}
    \newTransition{1}{2}{b}{}
    \newTransition{2}{3}{b}{}
    \newTransition{2}{1}{a}{bend left=45}
    \newTransition{3}{1}{a}{bend left=60}
    \newTransition{3}{0}{b}{bend right=45}
  \end{tikzpicture}
  \caption{DFA, accepting the regular expression: $(a | b)* abb$
  \label{fig:dfa}}
\end{figure}

 
