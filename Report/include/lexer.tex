\chapter{Lexer}
A lexer, lexical analyser, is a pattern matcher. Its job is to divide a text into a sequence of tokens (such as words, punctuation and symbols). The Lexer is a front end of a syntax 
analyser \cite{sebesta2012}. The syntax analyser in turn takes the tokens
generated by the lexer and returns a set of expressions and statements.
This can be done by using regular expressions, regular sets and finite
automata, which are central concepts in formal language theory \cite{Aho1990}.
The rest of this chapter describes the concepts of the lexer in detail.

\section{Lexing vs Parsing}
Lexers usually work as a pass before parser; giving their result to the syntax 
analyser.
There are several reasons why a compiler should be separated in to a lexical 
analyser and a parser (syntax analyser). 

First, simplicity of design is the most
important reason. When dividing the task into these two sub tasks, it allows the
programmer to simplify each of these sub-tasks. For example, a parser that has to 
deal with white-spaces and comments would be more complex 
than one that can assume these have already been removed by 
a lexer. When the two tasks have been seperated into sub-tasks it can lead to 
cleaner overall design when designing a new language.
The only thing the syntax analyser will see is the output from the 
lexer, tokens and lexemes.
The lexer usually skips comments and white-spaces, since these are not relevant 
for the syntax analyser.

Second, overall efficiency of the compiler can be improved. When separating the 
lexical analyser it allows for use of specialised techniques that can be used 
only in the lexical task.

Third and last, compiler portability can be enhanced. That is Input-device-specific 
peculiarities can be restricted to the lexical analysis \cite{Aho2006}.
Therefore the lexer can detect syntactical errors in tokens, such as ill-formed 
floating-points literals, and report these errors to the user \cite{sebesta2012}.
Finding these errors allows the compiler to break the compilation before
running the syntax analyser, thereby saving computing time. 
\section{Token Specification}
The job of the lexical analyser is to translate a human readable text to an abstract 
computer-readable list of tokens. There are different techniques a lexer can
use when finding the abstract tokens representing a text. This section describes the
techniques used when writing rules for the tokens patterns. 
\subsection{Regular Expressions}
\begin{example}[Valid C Idents \cite{Aho2006}]\label{regexpEx}
Using regular expressions to express a set of valid C identifiers is easy.
given an element $letter \in \{$a$ \dots $z$\} \cup \{$A$ \dots $Z$\} \cup 
\{$\_$\}$
and another element $digit \in \{0 \dots 9\}$
Then using a regular expression, the definition of all valid C identifiers 
could look like this: $letter (letter | digit)*$. 
\end{example}
\begin{definition}[Regular Expressions \cite{Aho1990}]\label{regexp} $ $\\
\begin{enumerate}
  \item The following characters are meta characters $meta = \{ '|', '(', ')', '*' \}$.
  \item A character $a \notin meta$ is a regular expression that matches the 
      string $a$.
  \item If $r_1$ and $r_2$ are regular expressions then $(r_1 | r_2)$ is a 
      regular expression that matches any string that matches $r_1$ or $r_2$.
  \item If $r_1$ and $r_2$ are regular expressions. $(r_1)(r_2)$ is a regular
      expression that matches the string $xy$ iff $x$ matches $r_1$
      and $y$ matches $r_2$.
  \item If $r$ is a regular expression $r*$ is a regular expression that
      matches any string of the form $x_1, x_2, \dots , x_n, n \geq 0$;
      where $r$ matches $x_i$ for $1 \leq i \leq n$, in particular $(r)*$ 
      matches the empty string, $\varepsilon$.
  \item If $r$ is a regular expression, then $(r)$ is a regular expression that
      matches the same string as $r$.
\end{enumerate}
\end{definition}
Many parentheses can be omitted by adopting the convention that the \emph{Kleene
closure} operator $*$ has the highest precedence, the \emph{concat} operator $(r_1)(r_2)$ the second highest and last the \emph{or}
operator $|$. The two binary operators, \emph{concat} and \emph{or} are
left-associative.
\subsection{Languages}
An alphabet is a finite set of symbols, for example Unicode, which 
includes approximately $100,000$ characters. A language is any 
countable set of strings of some fixed alphabet \cite{Aho2006}.
The term "formal language" refers to languages which can be described by a body 
of systematic rules. There is a subset of languages to formal languages called 
regular language, these regular languages refers to those languages 
that can be defined by regular expressions \cite{Ranta2012}.
\subsection{Regular Definitions}
When defining a language it is useful to give the regular expressions names, 
so they can for example be used in other regular expressions. These names for
the regular expressions are themselves symbols. If $\Sigma$ is an alphabet of 
basic symbols, then a regular definition is a sequence of definitions of the form:
\begin{center}
\begin{tabular}{l c r}
$d_1$ & $\to$ & $r_1$\\
$d_2$ & $\to$ & $r_2$\\
$\vdots$ & $\to$ & $\vdots$\\
$d_n$ & $\to$ & $r_n$\\

\end{tabular}
\end{center}
where:
\begin{enumerate}
\item Each $d_i$ is a new symbol, not in $\Sigma$ and not the same as any other 
of the $d$'s.
\item Each $r_i$ is a regular expression over the alphabet $\Sigma  \cup \{d_1, 
d_2 \dots d_{i-1}\}$
\end{enumerate}
By restricting $r_i$ to $\Sigma$ and previously defined $d$'s the regular 
definitions avoid recursive definitions \cite{Aho2006}.

\section{Tokens, Patterns and Lexemes}
When rules have been defined for a language, the lexer needs structures to
represent rules and the result from lexing the code-string. 
This section describe the structures which the lexical analyser use
for representing the abstract data; what these structures are for and what is 
forwarded to the syntactical analyser. 

A lexical analyser uses three different concepts. The concepts are described 
below. 
\begin{description}
  \item[Token]
    is a pair consisting of a token name and an optional attribute value. The 
token name is an abstract symbol corresponding to a lexical unit \cite{Aho2006}. 
For example, a particular keyword, data-type or identifier.
  \item[Pattern]
    is a description of what form a lexeme may take \cite{Aho2006}. 
For example, a keyword is just the sequence of characters that forms the 
keyword, an int is just a sequence consisting of just numbers. Can be described 
by a regular expression.
  \item[Lexemes]
    is a sequence of characters in the code being analysed which 
matches the pattern for a token and is identified by the lexical analyser as an 
instance of a token \cite{Aho2006}.
\end{description}
As mentioned before a token consists of a token name and an optional attribute value. 
This attribute is used when one pattern can match more then one lexeme.
For example the pattern for a digit token matches both $0$ and $1$, 
but it is important for the code generator to know which lexeme was found. 
Therefore the lexer often returns not just the token but also an attribute value 
that describes the lexeme found in the source program corresponding to this 
token \cite{Aho2006}.

\begin{figure}[h!]
\begin{center}
\begin{grammar}

<letter>  $\in$ \{`a' - `z'\} $\cup$ \{`A' - `Z'\} $\cup$ \{`_'\}

<digit>  $\in$ \{0 - 9\}

<identifier> ::= <letter> (<letter> | <digit>)* 

<integer> ::= <digit>+

<multi-line comment> ::= `/*' ([$\wedge$ `*'] | `*' [$\wedge$ `/'])* `*/'

<reserved-words> ::= `(' | `)' | `{' | `}' | `;' | `=' | `++' | `<' | `+' | `-' | `*'

\end{grammar}
\caption{Grammar rules for \cref{codeToToken} \& \cref{longestMatch}\label{fig:grammar}}
\end{center}
\end{figure}

A lexer collects chars into logical groups and assigns 
internal codes to these groups according to their structure, 
where the groups of chars are lexemes and the internal codes are tokens \cite{sebesta2012}.
In some cases it is not relevent to return a token for a pattern, in these
cases the token and lexeme is simply discarded and the lexer continues,
typical examples of this is whitespaces and comments which have no impact on
the code \cite{Aho2006}.

An example follows how a small piece of code would be divided given the regular
language described in \cref{reglang}.
\begin{example}[Logical grouping \cite{sebesta2012}] \label{codeToToken}$ $\\
Consider the following text; to be lexed:
\lstinputlisting[language=c]{examples/token.c}
Given the regular languaged defined in \cref{reglang}, the lexical analyser would use the rules defined in \cref{fig:grammar}
In order to produce the following tokens.
\begin{center}
\begin{tabular}{l c}
\underline{Token} & \underline{Lexeme}\\
Identifier & result\\
Reserved & $=$\\
Identifier & oldsum\\
Reserved & $-$\\
Identifier & value\\
Reserved & $/$\\
Integer & 100\\
Reserved & ;
\end{tabular}
\end{center}
\end{example}

\section{Recognition of Tokens}
In previous section the topic have been, how to represent a pattern using 
regular expressions and how these expressions relates to tokens. This section 
will highlight how to transform a sequence of characters into a sequence of 
abstract tokens. First giving some basic understanding with transition diagrams.  

\subsection{Transition Diagrams}
A state transition diagram, or just transition diagram is a directed graph,
where the nodes are labelled with state names. Each node 
represents a state which could occur during the process of scanning the input, 
looking for a lexeme that matches one of several patterns \cite{Aho2006}. The 
edges are labelled with the input characters that causes transitions among 
the states. An edge may also contain actions that the lexer must perform when the
transition is a token \cite{sebesta2012}.

Some properties of transition diagrams
follow. One state is said to be initial state. The transition 
diagram always begins at this state, before any input symbols have been read. 
Some states are said to be accepting (final). They indicate that a lexeme has 
been found. If the found token is the longest match (see \cref{longmatch}) then the token will be 
returned with any additional optional values, mentioned in previous section, 
and the transition is reset to the initial state \cite{Aho2006}.

\subsection{Longest Match}\label{longmatch}
If there are multiple feasible solutions when performing the lexical
analysis, the lexer will return the token that is the longest. To manage this
the lexer will continue in the transition diagram if there are any legal edges
leading out of the current state, even if it is an accepting state.\cite{Aho2006}.

The above rule is not always enough since the lexer has to explore all legal
edges, even if the current state is accepting. If the lexer is in a state that
is not accepting and don't have any legal edge out of that state, the lexer
can't return a token. To solve this the lexer has to keep track of what the
latest accepting state was. When the lexer reaches a state with no
legal edge out of it, the lexer returns the token corresponding to the last
accepting state. The tail of the string, the part that wasn't in the returned
token, is then lexed from the initial state as part of a new token.\cite{Aho2006}

\begin{example}[Longest Match] \label{longestMatch}
Consider the following text; to be lexed.
\lstinputlisting[language=c]{examples/longesttoken.c}
Allthough this text is not legal code, there is no lexical errors in it. Since
the text starts with a multi line comment sign the lexer will try to lex it as
a comment. When the lexer encounters the end of the text it will return the
token corresponding to the last accepting state and begin lexing the rest from
the initial state. The rules relevant to this example are defined in 
\cref{fig:grammar} the rest of the rules can be found in \cref{reglang}.\\
The result:
\begin{center}
\begin{tabular}{l c}
\underline{Token} & \underline{Lexeme}\\
Reserved & $/$\\
Reserved & $*$\\
Identifier & result\\
Reserved & $=$\\
Identifier & oldsum\\
Reserved & $-$\\
Identifier & value\\
Reserved & $/$\\
Integer & 100\\
Reserved & ;
\end{tabular}
\end{center}
\end{example}

\subsection{Finite Automata}
Transition diagrams of the formed used in lexers are representations of a class 
of mathematical machines called finite automata. Finite automata can be 
designed to recognise members of a class of languages called regular languages, 
mentioned above \cite{sebesta2012}.
A finite automaton is essentially a graph, like transitions diagrams, with some 
differences:
\begin{itemize}
  \item Finite automata are recognizers; they simply say "YES" or "NO" about 
each possible input string.
  \item Finite automata comes in two different forms:
    \begin{description}
      \item [Non-deterministic Finite Automata (NFA)] which have no restriction 
of the edges, several edges can be labelled by the same symbol out from the 
same state. Further $\epsilon$, the empty string, is a possible label. 
      \item [Deterministic Finite Automata (DFA)] for each state and for each 
symbol of its input alphabet exactly one edge with that symbol leaving that 
state. The empty string $\epsilon$ is not a valid label.
    \end{description}
\end{itemize}
Both these forms of finite automata are capable of recognising the same 
subset of languages, all regular languages \cite{Aho2006}.

\subsubsection{Non-deterministic Finite Automata}
An NFA accepts the input; $x$ if and only if there is a path in the transition 
diagram from the start state to one of the accepting states, such that the 
symbols along the way spells out $x$ \cite{Aho2006}.
The formal definition of a non-deterministic finite automaton follows:
\begin{definition}[Non-deterministic Finite Automata \cite{sipser2006}] \label{finiteAutomataDef}
A finite automata is a 5-tuple $(Q, \Sigma, \delta, q_0, F)$, where
\begin{enumerate}
  \item $Q$ is a finite set called the states,
  \item $\Sigma$ is a finite set called alphabet,
  \item $\delta: Q \times \Sigma \to P(Q)$ is a transition function,
  \item $q_0 \in Q$ is the start state, and
  \item $F \subseteq Q$ is the accept state.
\end{enumerate} 
\end{definition}
The transition function doesn't map to one particular state from a
state and element tuple. This is because one state may have more then one edge
per element. An example of this can be seen in \cref{regexp2td}.

There are two different ways of representing an NFA which this report will
describe. One is by transition diagrams, where the regular expression will be
represented by a graph structure. Another is by transitions table, where the 
regular expression will be converted in to a table of states and the 
transitions for these states given the input. The following examples shows how 
the transition diagram and transition table representation will look like for a 
given regular expression.

\begin{example}[RegExp to Transition Diagram \cite{Aho2006}] \label{regexp2td}
Given this regular expression:
\begin{center} $(a | b)* abb$ \end{center}
the transition diagram in \cref{fig:td} is representing this regular expression.
\end{example}
\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[
    % Default arrow tip
    ->,>=stealth',shorten >=1pt,auto,
    % Default node distance
    node distance=2cm,
    % Edge stroke thickness: semithick, thick, thin
    semithick
    ]

    \newState{0}{$0$}{initial}{}
    \newState{1}{$1$}{right of=0}{}
    \newState{2}{$2$}{right of=1}{}
    \newState{3}{$3$}{right of=2}{accepting} 

    \newTransition{0}{0}{a}{loop above}
    \newTransition{0}{0}{b}{loop below}
    \newTransition{0}{1}{a}{}
    \newTransition{1}{2}{b}{}
    \newTransition{2}{3}{b}{}
  \end{tikzpicture}
  \caption{Transition Diagram, accepting the pattern  $(a | b)* abb$ 
  \label{fig:td}}
  \end{figure}

\begin{example}[RegExp to Transition Table \cite{Aho2006}] \label{regexp2tt}
Given the regular expression from \cref{regexp2td} it can be converted into the transition table shown in \cref{fig:tt}
\end{example}
\begin{figure}[h!]
  \centering
  \begin{tabular}{| c | c c c |}
    \hline
    \hline
    State & a & b & $\epsilon$\\
    \hline
    0 & $\{0, 1\}$ & $\{0\}$ & $\emptyset$ \\
    1 & $\emptyset$ & $\{2\}$ & $\emptyset$ \\
    2 & $\emptyset$ & $\{3\}$ & $\emptyset$ \\
    3 & $\emptyset$ & $\emptyset$ & $\emptyset$ \\
    \hline
  \end{tabular}
  \caption{Transition Table representation of regular expression in 
        \cref{regexp2td} \label{fig:tt}}
\end{figure}
Transition tables have the advantage that they have an quick lookup time. But 
instead it will take a lot of data space, when the alphabet is large. Most 
states do not have any move on most of the input symbols \cite{Aho2006}.
\subsubsection{Deterministic Finite Automata}
DFA is a special case of an NFA where,
\begin{enumerate}
  \item there are no moves on input $\epsilon$ and
  \item for each state $s$ and input symbol $a$, there is exactly one edge out
        of $s$ labelled with $a$.
\end{enumerate}
While a NFA is an abstract representation of an algorithm to recognise the string 
of a language, the DFA is a simple concrete algorithm for recognising strings. 
Every regular expression can be converted in to a NFA and every NFA can be 
converted in to a DFA and then converted back to a regular expression \cite{Aho2006}. 
It is the DFA that is implemented and used when building lexical analysers.
The formal definition of a deterministic finite automaton follows:
\begin{definition}[Deterministic Finite Automata \cite{sipser2006}] \label{finiteAutomataDef}
A finite automata is a 5-tuple $(Q, \Sigma, \delta, q_0, F)$, where
\begin{enumerate}
  \item $Q$ is a finite set called the states,
  \item $\Sigma$ is a finite set called alphabet,
  \item $\delta: Q \times \Sigma \to Q$ is a transition function,
  \item $q_0 \in Q$ is the start state, and
  \item $F \subseteq Q$ is the set of accept states.
\end{enumerate} 
\end{definition}
\begin{example}[DFA representation of RegExp \cite{Aho2006}] \label{regexp2dfa}
A DFA representation of same regular expression from \cref{regexp2td} is shown in \cref{fig:dfa}
\end{example}
\begin{figure}[!h]
  \centering
  \begin{tikzpicture}[
    % Default arrow tip
    ->,>=stealth',shorten >=1pt,auto,
    % Default node distance
    node distance=2cm,
    % Edge stroke thickness: semithick, thick, thin
    semithick
    ]

    \newState{0}{$0$}{initial}{}
    \newState{1}{$1$}{right of=0}{}
    \newState{2}{$2$}{right of=1}{}
    \newState{3}{$3$}{right of=2}{accepting} 

    \newTransition{0}{0}{b}{loop above}
    \newTransition{0}{1}{a}{}
    \newTransition{1}{1}{a}{loop below}
    \newTransition{1}{2}{b}{}
    \newTransition{2}{3}{b}{}
    \newTransition{2}{1}{a}{bend left=45}
    \newTransition{3}{1}{a}{bend left=60}
    \newTransition{3}{0}{b}{bend right=45}
  \end{tikzpicture}
  \caption{DFA, accepting the regular expression: $(a | b)* abb$
  \label{fig:dfa}}
\end{figure}

 
