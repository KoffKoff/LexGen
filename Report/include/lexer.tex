\chapter{Lexer \label{chap:lexer}}
%Skriv om stycke.
A lexer, lexical analyzer, is a pattern matcher. Its job is to divide a text
into a sequence of tokens (such as words, punctuation and symbols). A Lexer is
often used as the first stage of a syntax analyzer \cite{sebesta2012}. The syntax
analyzer in turn takes the tokens generated by the lexer and returns a set of
expressions and statements. Lexing can be done by using regular expressions,
regular sets and finite automata, which are all fundamental notions in
formal language theory \cite{Aho1990}. The rest of this chapter describes the
concepts of the lexer in detail.

\section{Lexing vs Parsing}
It is common that a parser has a pre-step where the plain text is transformed
in to some more computer readable form. This step is performed by a lexical
analyzer.
The computer friendly output is then then given to the syntactical analyzer of
the parser. Splitting up a parser into a number of different tasks have several
benefits. Here follows some benefits from breaking out the lexical analyzer from
the syntactical analyzer.

To keep a clean design of the parser. If the lexical analysis is stripped out of
a parser, the parsing step can be designed in a cleaner way. The lexical
analyzer can also be designed in a cleaner and smarter way. A lexer can for
instance ignore to pass along unneeded (for the parser) data, like white spaces
and comments. This opens up for a cleaner design when defining a new programming
language. The syntactical part of the parser will only receive the output from
the lexer as input. The output of the lexer is described in more detail further
on in this chapter \cite{Aho2006}.

\newpage

Splitting a big problem into smaller specific sub-problems enables Methods that
are refined on specific tasks to be used. This can result in a more efficient
parser \cite{Aho2006}.

Breaking out the lexical analyzer from the parser makes it possible for the
syntactical analyzer to solve its problem in a generic way. When changing language
to parse, the only part that needs to be changed in the parser is the lexical
analyzer. This opens up for portability \cite{Aho2006}.

If there is an illegal character sequences inside the code it will be detected
by the lexer and feedback will be given to the user \cite{sebesta2012}. Because
the lexer can find and report these errors to the user, there is no need to go
into the syntactical analysis of the parser. Hence saving running time for
giving the user feedback.

\section{Token Specification}
As mentioned earlier in this chapter, the responsibility of the lexical analyzer
is to transform a human readable text to an abstract computer readable list of
tokens. There are different techniques a lexer can use when finding the
abstract tokens representing a text. This section describes the techniques used
when writing rules for the tokens patterns.

\subsection{Regular Expressions}
Regular expressions are used to verify if a sequence of symbols matches a
pattern. Due to the definition of regular expressions they cannot describe all
possible patterns. However, they are in most cases good enough for lexers.

\begin{figure}[hb!]
\begin{definition}[Regular Expressions \cite{Aho1990}]\label{regexp} $ $\\
\begin{enumerate}
  \item The following characters are meta characters: $meta = \{ '|', ~~ '(', ~~ ')', ~~ '*' \}$.
  \item A character $a \notin meta$ is a regular expression that matches the 
      string $a$.
  \item If $r_1$ and $r_2$ are regular expressions then $(r_1 | r_2)$ is a 
      regular expression that matches any string that matches $r_1$ or $r_2$.
  \item If $r_1$ and $r_2$ are regular expressions. $(r_1)(r_2)$ is a regular
      expression that matches the string $xy$ iff $x$ matches $r_1$
      and $y$ matches $r_2$.
  \item If $r$ is a regular expression $r*$ is a regular expression that
      matches any string of the form $(x_1)(x_2) \dots  (x_n), n \geq 0$;
      where $X_i$ matches $r$ for $1 \leq i \leq n$, in particular $(r)*$ 
      matches the empty string, $\varepsilon$.
  \item If $r$ is a regular expression, then $(r)$ is a regular expression that
      matches the same string as $r$.
\end{enumerate}
\qeda
\end{definition}
\end{figure}

Given the definition of regular expressions seen in \cref{regexp},
by introducing a priority level and associativity to the different operators,
parentheses can be eliminated. The operator with the highest priority is the $*$
operator. The second highest is the \emph{concat} operator $(r_1)(r_2)$ and the
operator with the lowest level is the \emph{or} operator $|$. The $*$ operator
can not have a associativity since it only takes one argument. The other two
binary operators \emph{concat} and \emph{or} are left-associative
\cite{Aho1990}.
\newline
\begin{example}[Valid C Idents \cite{Aho2006}]\label{regexpEx}
A valid C identifier must start with a letter character and then have zero or more characters or digits. To describe this an element $letter
\in \{$a$ \dots $z$\} \cup \{$A$ \dots $Z$\} \cup \{$\_$\}$ is intruduced and
another element $digit \in \{0 \dots 9\}$.

Then by using these elements the regular expression for describing all leagual C
identifiers can be design in the following way: $letter (letter | digit)*$.
\end{example}

\subsection{Languages}
A language is build up by an alphabet. An alphabet is represented by a finite
collection of characters. These symbols can build up strings and a language is
a countable set of these different strings \cite{Aho2006}. For example the
alphabet Unicode used by computers to represent text includes over $100,000$
different symbols. This means that a language can be enormous.

There are different types of languages. A lexer can work with a subset of all
languages which can be described by a set of systematic rules, these are called
formal languages. The lexer can however not work with all formal languages, only
with the languages which can be described by regular expressions, these are
called regular languages \cite{Ranta2012}.

\subsection{Regular Definitions}
To be able to reuse already written expression, an identifier $d$ can be
assigned to an expression. $d$ can then be used in expressions. However
this introduces the problem of recursive definitions. To counteract this
the properties for the identifiers and expressions are defined as follows.

A set of regular definitions for an alphabet $\Sigma$ is given, which can be
seen in \cref{fig:regularDefinitions}

\begin{figure}[hb!]
\begin{center}
\begin{tabular}{l c r}
$d_1$ & $\to$ & $r_1$\\
$d_2$ & $\to$ & $r_2$\\
$\vdots$ & $\to$ & $\vdots$\\
$d_n$ & $\to$ & $r_n$\\
\end{tabular}
\caption{List of definitions and their regular expressions \label{fig:regularDefinitions}}
\end{center}
\end{figure}

The following property apply on the definition identifiers $d_1 ... d_n$: a
definition identifier $d_i$ is a new symbol not already present in the alphabet
$\Sigma$ and not equal to any other definition identifier
$d_x \in {d_1 ... d_n}$ where $i \neq x$.
The following property apply on the regular expressions $r_1 ... r_n$: a regular
expression $r_i$ can work on the alphabet $\Sigma \cup {d_1 ... d_{i-1}}$
\cite{Aho2006}.

\section{Tokens, Patterns and Lexemes}
When rules have been defined for a language, the lexer needs structures to
represent the rules and the result from lexing the text.
This section describes the structures which the lexical analyzer uses
for representing the abstract data; what these structures are used for and what is
forwarded to the syntactical analyzer.

The following three different structural concepts are vital to the lexical
analyzer:
\begin{itemize}
  \item A \textbf{token}, which is an abstract for representing an atomic code
  segment. The token is represented by a name and an optional attribute for
  holding the value of the token \cite{Aho2006}.
  \item A \textbf{pattern}, which is the regular expression for describing the
  text format on which a token can be represented \cite{Aho2006}. For example a
  string in most languages is represented by first a " character and zero or 
  more characters and finally ends with a " character.
  \item A \textbf{lexeme}, which is the text fulfilling the pattern bound to 
  the token. Therefore a lexeme can be viewed as an instance of an abstract 
  token \cite{Aho2006}.
\end{itemize}
As mentioned before, a token can carry an optional attribute. When a token can
be represented by several different instances, lexemes, this attribute is used
for giving the specific value. For example a string token can be represented by
the different strings "", "a", "b" and so on. However different parts of the
compiler may need to know the exact string token instance which was found
Therefore the lexer need to pass the information further on \cite{Aho2006}.

To summarize, a lexer reads characters from a code and finds the largest 
continues sequences which builds up valid tokens \cite{sebesta2012}. As 
mentioned before it is not always relevant to return an attribute to the token. 
These cases can be when finding keywords of the language, like in $if$, $for$
and $while$. There are cases when it make no sense to return the found token.
Example of such cases could be tokens for comments and white-spaces, which in 
most languages has no relevance to the compiled code. In these cases the lexer 
just drops the token and continues the lexical routine \cite{Aho2006}.
\cref{codeToToken} shows how a small piece of code is divided in to abstract
tokens using the rules in \cref{fig:grammar}. The complete language specification
can be found in \cref{reglang}.

\newpage

\begin{figure}[h!]
\begin{addmargin}[2em]{0em}
\begin{grammar}

<letter>  $\in$ \{`a' - `z'\} $\cup$ \{`A' - `Z'\} $\cup$ \{`_'\}

<digit>  $\in$ \{0 - 9\}

<identifier> ::= <letter> (<letter> | <digit>)* 

<string> ::= `"' [$\wedge$ `"']* `"'

<multi-line comment> ::= `/*' ([$\wedge$ `*'] | `*' [$\wedge$ `/'])* `*/'

<reserved-words> ::= `(' | `)' | `{' | `}' | `;' | `=' | `++' | `<' | `+' | `-' | `*'

\end{grammar}
\end{addmargin}
\caption{Grammar rules for \cref{codeToToken} \& \cref{longestMatch}\label{fig:grammar}}
\end{figure}

\begin{example}[Logical grouping \cite{sebesta2012}] \label{codeToToken}$ $\\
Consider the following text; to be lexed:
\lstinputlisting[language=c]{examples/token.c}
Given the regular language defined in \cref{reglang}, the lexical analyzer would
use the rules defined in \cref{fig:grammar} and produce the resulting
tokens shown in \cref{fig:codeToToken}.

\begin{figure}[h!]
\begin{center}
\begin{tabular}{l c}
\underline{Token} & \underline{Lexeme}\\
Identifier & fileName\\
Reserved & $=$\\
Identifier & filePath\\
Reserved & $+$\\
String & ".png"\\
Reserved & ;
\end{tabular}
\end{center}
\caption{Result of lexing the code in \cref{codeToToken} \label{fig:codeToToken}}
\end{figure}
\end{example}

\section{Recognition of Tokens}
The previous section showed how Regular expressions can be used
to express patterns for tokens. In this section the different techniques on how
to transform a sequence of characters into abstract tokens using these patterns
is described.

\subsection{Transition Diagrams}
To recognize tokens from a pattern transitions diagrams can be used, these are
directed graphs consisting of nodes and edges. The nodes correspond to the
discrete states in the transformation process. In the transition diagram there
are three kinds of states. The first is the the starting state, there can only
be one starting state in the graph. It is from this state the process start when
a new token should be found. There are at least one accepting state, these states
represents that a valid token has been found. Then there can be zero or more
none-accepting states. These states represents that a token has not yet been
found \cite{Aho2006}.

The edges in the graph are represented by the input character which must be
found to be able to traverse between the two states which the edge connects. If
there is no valid edge out of an accepting state, the found token is said to be
the longest match (see \cref{sub:longmatch}) then that token is returned and the
lexer starts reading the next character from the starting state \cite{Aho2006}.

\subsection{Longest Match}\label{sub:longmatch}
If there are multiple feasible solutions when performing the lexical
analysis, the lexer will return the token that is the longest. To manage this
the lexer will continue in the transition diagram if there are any legal edges
leading out of the current state, even if it is an accepting state \cite{Aho2006}.

The above model introduces a new problem. If the lexer ends up in a state that
is not accepting and do not have any legal edge out of that state, the lexer
can not return a token. To solve this the lexer has to keep track of what the
latest accepting state was. When the lexer reaches a state with no
legal edge out of it, the lexer returns the token corresponding to the last
accepting state. The tail of the string, the part that was not in the returned
token, is then lexed from the initial state as part of a new token
\cite{Aho2006}.

\begin{figure}[hb!]
\begin{center}
\begin{tabular}{l c}
\underline{Token} & \underline{Lexeme}\\
Reserved & $/$\\
Reserved & $*$\\
Identifier &fileName\\
Reserved & $=$\\
Identifier & filePath\\
Reserved & $+$\\
String & ".png"\\
Reserved & ;
\end{tabular}
\end{center}
\caption{Result of lexing the code in \cref{longestMatch} \label{fig:longestmatch}}
\end{figure}
\newpage
\begin{example}[Longest Match] \label{longestMatch}
Consider the following text; to be lexed.
\lstinputlisting[language=c]{examples/longesttoken.c}
Although this piece of C code is not syntactically correct, there are no
lexical errors in it. Since
the text starts with a multi line comment sign the lexer will try to lex it as
a comment. When the lexer encounters the end of the text it will return the
token corresponding to the last accepting state and begin lexing the rest from
the initial state. The rules relevant to this example are defined in 
\cref{fig:grammar} the rest of the rules can be found in \cref{reglang}.\\
The result can be found in \cref{fig:longestmatch}.
\end{example}

\subsection{Finite Automata}
To recognize members of regular languages, which are languages that lexers can
be used with, a mathematical machine called finite automata can be used
\cite{sebesta2012}. Finite automata are purely recognizers, they only say if an
input sequence is valid or not.

There are two different forms of finite automata, which both are capable of
working on regular languages \cite{Aho2006}:
\begin{description}
\item [Non-deterministic Finite Automata (NFA):] As the name suggest there are no
requirements of a deterministic path for an input sequence in this type of
automata. A state may have multiple edges for the same symbol. Also edges my
take no symbol, the empty string $\epsilon$.
\item [Deterministic Finite Automata (DFA):] In this form there can only be one
path for an input. That is, a state must have exactly one edge per input symbol
leaving the state and edges are not allowed to have the empty string $\epsilon$
as symbol.
\end{description}

There are two common ways of representing a finite automata, transition diagram
and transition table. A transition diagram is a directed graph where the nodes
are the states in the automata and the edges represent the symbol needed for the
next state. A transition table is a table where the rows represents the current
state, the columns the next symbol and the cell is the next state. Examples of
how a NFA can be described by these can be seen in \label{regexp2td}
\cite{Aho2006}.

\subsubsection{Non-deterministic Finite Automata}
a NFA will accept a string if there exists atleast one path from the starting
state to an accepting state where the edges along the path spell out the symbols
in the string \cite{Aho2006}.
The formal definition of a non-deterministic finite automaton follows:
\newpage
\begin{definition}[Non-deterministic Finite Automata \cite{sipser2006}] \label{finiteAutomataDef}
A finite automata is a 5-tuple $(Q, \Sigma, \delta, q_0, F)$, where
\begin{enumerate}
  \item $Q$ is a finite set called the states,
  \item $\Sigma$ is a finite set called alphabet,
  \item $\delta: Q \times \Sigma \to P(Q)$ is a transition function,
  \item $q_0 \in Q$ is the start state, and
  \item $F \subseteq Q$ is the accepting states.
\end{enumerate} 
\end{definition}

\cref{regexp2td} shows how the transition diagram and transition table
representation will look like for a given regular expression. Since a state can
have several edges with the same symbol, the transition function does not map to
a single state.
\newline
\begin{example}[RegExp to Transition Diagram \& Transition Table \cite{Aho2006}] \label{regexp2td}
Given the regular expression $(a|b|c)(a|c)* c$ a transition diagram can be
created that represents the expression, see \cref{fig:td}. The transition table
in \cref{fig:tt} represents the same graph.

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[
    ->,>=stealth',shorten >=1pt,auto,
    node distance=2cm,
    semithick
    ]

    \newState{0}{$0$}{initial}{}
    \newState{1}{$1$}{right of=0}{}
    \newState{2}{$2$}{right of=1}{accepting}

    \newTransition{0}{1}{a}{bend left=45}
    \newTransition{0}{1}{b}{}
    \newTransition{0}{1}{c}{bend right=45}
    \newTransition{1}{1}{a}{loop above}
    \newTransition{1}{1}{c}{loop below}
    \newTransition{1}{2}{c}{}
  \end{tikzpicture}
  \caption{Transition Diagram, accepting the pattern $(a|b|c)(a|c)* c$
  \label{fig:td}}
\end{figure}

\begin{figure}[h!]
  \centering
  \begin{tabular}{| c | c c c c |}
    \hline
    \hline
    State & a & b & c & $\epsilon$\\
    \hline
    0 & $\{1\}$ & $\{1\}$ & $\{1\}$ & $\emptyset$ \\
    1 & $\{1\}$ & $\emptyset$ & $\{1,2\}$ & $\emptyset$ \\
    2 & $\emptyset$ & $\emptyset$ & $\emptyset$ & $\emptyset$ \\
    \hline
  \end{tabular}
  \caption{Transition Table, accepting the pattern $(a|b|c)(a|c)* c$
  \label{fig:tt}}
\end{figure}
\end{example}

Transition tables store all possible transitions which gives it a quick lookup
time. However there are often a majority of states which does not have any
transitions for some input symbols. And since the table stores all states it
will need a lot of data space, especially for situations when the alphabet for
the language is large \cite{Aho2006}.

\subsubsection{Deterministic Finite Automata}
DFA is a special case of an NFA where, edges can not be labeled with the empty
input $\epsilon$ and there is exactly one edge for each symbol in the alphabet
out of every state.

If a NFA can be seen as the abstract form of a string-recognizer algorithm, a
DFA could be seen as a concrete algorithm for finding a specific string. As
mentioned Finite Automata can be generated from regular expressions. That is, a
NFA can be generated from regular expressions and a DFA can be generated from a
NFA. This goes the other way as well, and DFA can be converted into a regular
expression \cite{Aho2006}. A lexer uses a DFA as the algorithm for pattern a
lexeme to a specific token.

The formal definition of a deterministic finite automaton follows:
\newline
\begin{definition}[Deterministic Finite Automata \cite{sipser2006}] \label{finiteAutomataDef}
A finite automata is a 5-tuple $(Q, \Sigma, \delta, q_0, F)$, where
\begin{enumerate}
  \item $Q$ is a finite set called the states,
  \item $\Sigma$ is a finite set called alphabet,
  \item $\delta: Q \times \Sigma \to Q$ is a transition function,
  \item $q_0 \in Q$ is the start state, and
  \item $F \subseteq Q$ is the set of accepting states.
\end{enumerate} 
\end{definition}
\hfill 
\break
\begin{example}[DFA representation of RegExp \cite{Aho2006}] \label{regexp2dfa}
A DFA representation of the regular expression from \cref{regexp2td} is shown in \cref{fig:dfa}
\end{example}
\begin{figure}[!h]
  \centering
  \begin{tikzpicture}[
    % Default arrow tip
    ->,>=stealth',shorten >=1pt,auto,
    % Default node distance
    node distance=2cm,
    % Edge stroke thickness: semithick, thick, thin
    semithick
    ]

    \newState{0}{$0$}{initial}{}
    \newState{1}{$1$}{right of=0}{}
    \newState{2}{$2$}{right of=1}{accepting} 

    \newTransition{0}{1}{a}{bend left=45}
    \newTransition{0}{1}{b}{bend right=45}
    \newTransition{0}{1}{c}{}
    \newTransition{1}{1}{a}{loop above}
    \newTransition{1}{2}{c}{}
    \newTransition{2}{1}{a}{bend left=45}
    \newTransition{2}{2}{c}{loop above}
  \end{tikzpicture}
  \caption{DFA, accepting the regular expression: $(a|b|c)(a|c)* c$
  \label{fig:dfa}}
\end{figure}
